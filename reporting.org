# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+TITLE: Internship with Benoit Baudry
#+DATE:
#+AUTHOR: Simon Bihel
#+EMAIL: [[mailto:simon.bihel@ens-rennes.fr]]
#+WEBSITE: [[simonbihel.me]]
#+LINK: [[https://github.com/sbihel/internship_amplification]]
#+LANGUAGE: en

* Introduction

* Findings
** Bibliography
*** Writing

*** References

** Contribution

* Development

* Global Goals

* Journal
** <2017-09-18 Mon>--<2018-02-07 Wed>
*** Things Done
- Meeting with Benoit
  + [[https://github.com/STAMP-project/dspot/issues/187][1]], [[https://github.com/STAMP-project/dspot/issues/129][2]], [[https://github.com/STAMP-project/dspot/issues/54][3]] issues for possible work to do
  + 1 possible work: explain if a mutant isn't killed because of oracle or input
  + focus on mutation (e.g. mutation score)
  + work will be on [[https://github.com/STAMP-project/dspot][Dspot]] and [[https://github.com/STAMP-project/pitest-descartes][PIT]].
- Read /Search-based software testing: Past, present and future/
  ([[http://mcminn.io/publications/c18.pdf][mcminn2011search]])
  + Already read from previous internship
- Read /B-Refactoring: Automatic Test Code Refactoring to Improve Dynamic Analysis/
  ([[https://hal.archives-ouvertes.fr/hal-01309004/file/banana-refactoring.pdf][xuan2016b]])
  + Split tests for each fragment to cover a simple part of the control flow.
  + Help with respect to fault localization.
- Read /Test data regeneration: generating new test data from existing test data/
  ([[http://www0.cs.ucl.ac.uk/staff/mharman/stvr-regeneration.pdf][yoo2012test]])
  + TODO
- Read /The Emerging Field of Test Amplification: A Survey/
  ([[https://arxiv.org/pdf/1705.10692.pdf][danglot2017emerging]])
  + Dense
  + Good overview of goals (Table 1) and methods (Table 2)
- Read /Erlang Code Evolution Control/
  ([[https://arxiv.org/pdf/1709.05291.pdf][arXiv:1709.05291]])
  + TODO
- Read /Genetic Improvement of Software: a Comprehensive Survey/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911210][petke2017genetic]])
  + TODO
  + [[http://www.cs.bham.ac.uk/~wbl/biblio/][http://www.cs.bham.ac.uk/~wbl/biblio/]]
- Read [[http://massol.myxwiki.org/xwiki/bin/view/Blog/MutationTestingDescartes][blog on PIT and Descartes]]
  + Sum up PIT/Descartes
  + List of wanted features
- Read /DSpot: Test Amplification for Automatic Assessment of Computational Diversity/
  ([[https://arxiv.org/pdf/1503.05807.pdf][baudry2015dspot]])
  + Comparison with TDR (yoo2012test) and also concurent to
    carzaniga2015measuring
    - "the key differences between DSpot and TDR are: TDR stacks multiple
      transformations together; DSpot has more new transformation operators on
      test cases: DSpot considers a richer observation space based on arbitrary
      data types and sequences of method calls."
    - "We count the number of variants that are identified as computationally
      different using DSpot and TDR. "
- Read /Automatic Software Diversity in the Light of Test Suites/
  ([[https://arxiv.org/pdf/1509.00144.pdf][baudry2015automatic]])
  + analysis of common features (e.g. number of tests covering one statement)
  + plastic behavior (have different behaviors while still remaining correct)
    study
  + different details compared to baudry2015dspot and baudry2014tailored
- Read /Measuring software redundancy/
  ([[https://pdfs.semanticscholar.org/0a93/144638ebfc924550798b620835a3fc9785cf.pdf][carzaniga2015measuring]])
  + TODO
- Read /Spoon: A Library for Implementing Analyses and Transformations of Java Source Code/
  ([[https://hal.archives-ouvertes.fr/hal-01078532v2/document][pawlak2016spoon]])
  + let's say it's like llvm/clang for now
- Read /Tailored source code transformations to synthesize computationally diverse program variants/
  ([[https://arxiv.org/pdf/1401.7635][baudry2014tailored]])
  + More details than we can find in baudry2015dspot
- Meeting with Benoit
  + The purpose of DSpot has shifted right?
    - interesting to talk about the history in bibliography? No, there is a new
      paper
  + Enough space to talk about related work? present a few papers in details and
    cite others
  + Current organisation of bibliography
    - General techniques
      + Definitions
      + Mutants
      + etc
    - Useful tools
      + DSpot
  + do extensive evaluation (comparison from scratch vs amplification)
  + find literals to help tests
  + add mutation operator for specific data structures
  + stack mutations
  + add explanations
  + 3 big open problems
- Read /A Systematic Literature Review on Test Amplification/
  + TODO
- Read /Genetic-Improvement based Unit Test Amplification for Java/
  + TODO
- Read /Evosuite/
  ([[http://www.evosuite.org/evosuite/][fraser2011evosuite]])
  + State-of-the-art tool
  + Very sophisticated, difficult to reproduce experiments because it changes
    fast and a lot of parameters are tweaked
- Read /Dynamic Analysis can be Improved with Automatic Test Suite Refactoring/
  ([[https://arxiv.org/pdf/1506.01883.pdf][xuan2015dynamic]])
  + TODO
- Meeting with Benoit
  + reduce only the generated tests
  + big question: minimal generated tests
    - pre or post treatement
    - order of presenting PRs
    - this is the big question
    - we don't want to touch the original suite
    - we want the programmer to understand the new tests
  + add an example of junit test
  + talk about the trend of genetic improvement
  + don't necesseraly cite /Automatic software diversity in the light of test
    suites/ and /Tailored source code transformations to synthesize
    computationally diverse program variants/
- Read /A Few Billion Lines of Code Later/
  ([[https://pdfs.semanticscholar.org/295f/4ffa651675b22ae8e2f3f30b400330da0c69.pdf][bessey2010few]])
  + Great to understand the limits of static analysis but also some of the
    limits of all analysis
  + Difficult to analyze code because of the diversity of build automation tools
  + "By default, companies refuse to let an external force modify anything."
  + "A misunderstood explanation means the error is ignored or, worse,
    transmuted into a false positive."
  + Many standards
  + Some people don't care about bugs, sometimes improving the tool reveals more
    bugs which is bad for the manager
- Talk rehearsal <2018-01-28 Mon 08:30>, notes by Vladislav
  - More illustrations (workflow graph?)
  + Check the test case example (too complicated for not much, not really java)
  + Year and conference acronym in footcite
  + Careful with lambdas for TDR (check with supervisor)
  + More details on commits/pull requests and emphasize the importance of
    developers reviewing generated tests
  + Slide 10 -> ugly (different spacings)
  + Stacking operators: explanation too sparse
  + 4th point in conclusion slide too vague. Not just the goal but also the mean
    to achieve it
- Read /Automatic Test Case Optimization: A Bacteriologic Algorithm/
  ([[https://www.researchgate.net/profile/Jean-Marc_Jezequel/publication/3248230_Automatic_Test_Case_Optimization_A_Bacteriologic_Algorithm/links/0912f50ca4c15eb416000000.pdf][baudry2005automatic]])
  + TODO
*** Blocking Points
*** Planned Work
- [X] Read papers
- [X] Meeting with Benoit <2017-09-22 Fri 15:00-15:30>
- [X] Meeting with Benoit <2017-11-23 Thu 15:00-16:00>
- [X] Send link to repo
- [X] Ask Maud about plane tickets refund
- [X] Meeting with Benoit <2017-12-22 Fri 10:30-11:30>

* Conclusion
