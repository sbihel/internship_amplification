# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer hidestars
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
# #+INFOJS_OPT: view:info toc:nil

#+TITLE: Internship with Benoit Baudry at KTH
#+SUBTITLE: Adaptation of Amplified Unit Tests for Human Comprehension
#+DATE: <2018-02-07 Wed>--<2018-06-22 Fri>
#+AUTHOR: Simon Bihel
#+EMAIL: [[mailto:simon.bihel@ens-rennes.fr]]
#+WEBSITE: [[simonbihel.me]]
#+LINK: [[https://github.com/sbihel/internship_amplification]]
#+LANGUAGE: en
#+KEYWORDS: labbook, internship


* Introduction


* Findings
** Bibliography
*** Writing
**** Context

**** Problem statement
Amplified tests have many new assertions and new inputs and DSpot keeps tests
depending on the fact that they detect new mutants. What that means is that
there is no enquiry on the usefulness of each amplification. Because the number
of new mutants killed is often times significantly lower than the number of
amplifications --- especially the number of assertions which are the one to
effectively detect a mutant --- we end up with a lot of useless statements. This
noise is problematic because it makes the review process longer, and because the
less focused a program is, the harder it is to comprehend. And noise is not the
only threat to focus as the new mutants can be completely different and from a
different part of the SUT compared to the ones the original test case could
detect. The final step towards a human-friendly output would be to add context
and human-like description of the amplification. To sum-up, the three identified
problems to tackle are noise reduction, focus confinement, and natural language
description of the amplification.

**** Technical Difficulties
In order to describe a mutant, you need information about it. You could give the
mutator "category" but you only have its class name. You could give the column
to the statement it was applied to to highlight it, maybe, but you do not have
access to that. You could use the position in the AST, but you do not know that.

Knowing the which assertion killed what mutant is essential. Be it to start a
program slice from that assertion, or simply paraphrasing the assertion to
explain what bug is detected. But you do not know that. And you barely know, with
ugly comments, what assertions are the result of amplifications. Identifying,
afterwards, the role that assertions play is cumbersome. You can run the test
case with every mutants. But first, you do not have directly access to mutants.
And what do you want to do with them? Instrument, by adding a probe after each
assertion? How do you automate elegantly such stream of test execution? Maybe
you can remove assertions, one by one, and see if mutants keep getting killed? I
know we are in SBSE but that is quite ugly.

As said before we have no direct information on the position of amplifications
in the new test case. Makes it harder to generate descriptions or apply
minimization on them. But what data structure would you use? Bookkeeping of the
position in the AST? How would you keep it up-to-date with multiple rounds of
amplifications?

**** On the usefulness of works from (code maintenance|software artifacts summarization)
A lot of effort has been put in generating human friendly descriptions for
various kinds of software artifacts. In particular, there have been works on
generating documentation (or comments or description or summary) for source
code, methods, and more interestingly unit test cases. These tools can generate
natural language description of /what/ the piece of code does and identify to
most important parts or lines of code. But as for /why/ a code change was done
or the role a piece of code plays --- i.e. understand the intentions of the
developers --- it is harder and tools need additional information or limit the
scope by identifying stereotypes (e.g. labelling a commit as Feature Addition).

But those works are not directly useful for our problem. First, we know why an
amplified test was kept, it is because it can detect a new bug.

**** On the usefulness of works from test cases minimization
Using delta-diff we can identify useless statement and then remove them. But
more powerful program minimisation tools are available. While we could predict
that the more minimisation is applied, the less code there is left to describe
thus the description is easier to generate, it is not obvious and others details
have to be taken into account.

First we might not want to modify the original part, as the developer might
already be familiar with it and it might be less overwhelming to grasp the
purpose of the test case. And even if the developer has never seen this test
before, a hand-written program is probably easier to understand than a compact
version.

And tools probably cannot be told not to touch certain parts.

**** What do we then propose as contribution


*** References
**** Cultural
- /Search Based Software Engineering: Techniques, Taxonomy, Tutorial/
  ([[https://www.researchgate.net/profile/Mark_Harman/publication/221051156_Search_Based_Software_Engineering_Techniques_Taxonomy_Tutorial/links/0046352052592d5c2c000000/Search-Based-Software-Engineering-Techniques-Taxonomy-Tutorial.pdf][harman2012search]])
  + TODO
- /A Few Billion Lines of Code Later/
  ([[https://pdfs.semanticscholar.org/295f/4ffa651675b22ae8e2f3f30b400330da0c69.pdf][bessey2010few]])
  + Great to understand the limits of static analysis but also some of the
    limits of all analysis
  + Difficult to analyze code because of the diversity of build automation tools
  + "By default, companies refuse to let an external force modify anything."
  + "A misunderstood explanation means the error is ignored or, worse,
    transmuted into a false positive."
  + Many standards
  + Some people don't care about bugs, sometimes improving the tool reveals more
    bugs which is bad for the manager
- /Spoon: A Library for Implementing Analyses and Transformations of Java Source Code/
  ([[https://hal.archives-ouvertes.fr/hal-01078532v2/document][pawlak2016spoon]])
  + let's say it's like llvm/clang for now
- /Regression Testing Minimisation, Selection and Prioritisation : A Survey/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.8696&rep=rep1&type=pdf][yoo2012regression]])
  + TODO
- /Clustering Test Cases to Achieve Effective & Scalable Prioritisation Incorporating Expert Knowledge/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.9479&rep=rep1&type=pdf][yoo2009clustering]])
  + TODO
- /Measuring software redundancy/
  ([[https://pdfs.semanticscholar.org/0a93/144638ebfc924550798b620835a3fc9785cf.pdf][carzaniga2015measuring]]) <<carzaniga2015measuring>>
  + TODO
- /Automatic Software Diversity in the Light of Test Suites/
  ([[https://arxiv.org/pdf/1509.00144.pdf][baudry2015automatic]])
  + analysis of common features (e.g. number of tests covering one statement)
  + plastic behavior (have different behaviors while still remaining correct)
    study
  + different details compared to [[baudry2015dspot]] and [[baudry2014tailored]]
- /Tailored source code transformations to synthesize computationally diverse program variants/
  ([[https://arxiv.org/pdf/1401.7635][baudry2014tailored]]) <<baudry2014tailored>>
  + More details than in [[baudry2015dspot]]

**** Search-based Software Testing
- /Search-based software testing: Past, present and future/
  ([[http://mcminn.io/publications/c18.pdf][mcminn2011search]])
  + Already read from previous internship
- /Genetic Improvement of Software: a Comprehensive Survey/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911210][petke2017genetic]])
  + TODO
  + [[http://www.cs.bham.ac.uk/~wbl/biblio/][http://www.cs.bham.ac.uk/~wbl/biblio/]]
- /Evosuite/ ðŸŒŸ
  ([[http://www.evosuite.org/evosuite/][fraser2011evosuite]]) ([[https://pdfs.semanticscholar.org/df36/d5c8c8ecace7f5b9347a0880daf2c10b3d4b.pdf][fraser2013evosuite]])
  + State-of-the-art tool
  + Very sophisticated, difficult to reproduce experiments because it changes
    fast and a lot of parameters are tweaked
  + minimization
    - remove unnecessary statements
    - careful not to generate long test cases
- /An Approach to Test Data Generation for Killing Multiple Mutants/ ðŸŒŸ
  ([[http://ieeexplore.ieee.org/abstract/document/4021328/][liu2006approach]])

**** Test Amplification
- /B-Refactoring: Automatic Test Code Refactoring to Improve Dynamic Analysis/
  ([[https://hal.archives-ouvertes.fr/hal-01309004/file/banana-refactoring.pdf][xuan2016b]])
  + Split tests for each fragment to cover a simple part of the control flow.
  + Help with respect to fault localization.
- /Test data regeneration: generating new test data from existing test data/
  ([[http://www0.cs.ucl.ac.uk/staff/mharman/stvr-regeneration.pdf][yoo2012test]]) <<yoo2012test>>
- /The Emerging Field of Test Amplification: A Survey/
  ([[https://arxiv.org/pdf/1705.10692.pdf][danglot2017emerging]])
  + Dense
  + Good overview of goals (Table 1) and methods (Table 2)
- /DSpot: Test Amplification for Automatic Assessment of Computational Diversity/
  ([[https://arxiv.org/pdf/1503.05807.pdf][baudry2015dspot]]) <<baudry2015dspot>>
  + Comparison with TDR [[yoo2012test]] and also concurrent to
    [[carzaniga2015measuring]]
    - "the key differences between DSpot and TDR are: TDR stacks multiple
      transformations together; DSpot has more new transformation operators on
      test cases: DSpot considers a richer observation space based on arbitrary
      data types and sequences of method calls."
    - "We count the number of variants that are identified as computationally
      different using DSpot and TDR. "
- /A Systematic Literature Review on Test Amplification/ ðŸŒŸ
  + TODO
- /Genetic-Improvement based Unit Test Amplification for Java/ ðŸŒŸ
  + TODO
- /Dynamic Analysis can be Improved with Automatic Test Suite Refactoring/
  ([[https://arxiv.org/pdf/1506.01883.pdf][xuan2015dynamic]])
  + TODO
- /Automatic Test Case Optimization: A Bacteriologic Algorithm/
  ([[https://www.researchgate.net/profile/Jean-Marc_Jezequel/publication/3248230_Automatic_Test_Case_Optimization_A_Bacteriologic_Algorithm/links/0912f50ca4c15eb416000000.pdf][baudry2005automatic]])
  + TODO
  + Compared to DSpot, no assertions generation, small programs.

**** Generating natural language descriptions for software artifacts
***** Surveys
- /Survey of Methods to Generate Natural Language from Source Code/ ðŸŒŸ
  ([[http://www.languageandcode.org/nlse2015/neubig15nlse-survey.pdf][neubig2016survey]])
  1. Survey papers
    - recommends [[nazar2016summarizing]]
  2. Generation Methods
    1. manual rules/templates
      + SWUM [[hill2009automatically]]&[[sridhara2010towards]]
        - test cases [[zhang2011automated]] & [[kamimura2013towards]]
        - changes [[buse2010automatically]] & [[cortes2014automatically]]
        - exceptions [[buse2008automatic]]
      - multiple lines description [[sridhara2011automatically]]
        + not useful, too high level
      - using execution path information [[buse2008automatic]] & [[zhang2011automated]]
        + not useful(?)
  3. +Content Selection Methods+
  4. +Targeted Software Units+
  5. +Training Data Creation+
  6. Evaluation
    - TODO later
- /Summarizing Software Artifacts: A Literature Review/ ðŸŒŸ
  ([[https://link.springer.com/content/pdf/10.1007%2Fs11390-016-1671-1.pdf][nazar2016summarizing]]) <<nazar2016summarizing>>
  + very complete

***** Tools for tests
- /Automatically Documenting Software Artifacts/ ðŸŒŸ
  ([[http://www.cs.wm.edu/~denys/pubs/dissertations/Boyang-thesis.pdf][li2018automatically]])
  + PhD thesis
  + Chapter 4 (p. 109) on tag for unit tests
  + catalog of 21 stereotypes for methods in unit tests
    - 14 JUnit API-Based Stereotypes for Methods in Unit Test Cases
      + Boolean verifier
      + Null verifier
      + Equality verifier
      + Identity verifier
      + Utility verifier
      + Exception verifier
      + Condition Matcher
      + Assumption setter
      + Test initializer
      + Test cleaner
      + Logger
      + Ignored method
      + Hybrid verifier
      + Unclassified
    - 7 C/D-Flow Based Stereotypes for Methods in Unit Test Cases
      + Branch verifier
      + Iterative verifier
      + Public field verifier
      + API utility verifier
      + Internal call verifier
      + Execution tester
      + Empty tester
- /Automatically Documenting Unit Test Cases/ ðŸŒŸðŸŒŸ
  ([[http://www.cs.wm.edu/~denys/pubs/_ICST'16-JUnitTestScribe-CRC.pdf][li2016automatically]]) ([[https://github.com/boyangwm/UnitTestScribe][git]])
  + Survey with developers and projects mining study to justify automatic
    documentation of unit tests
  + uses a SWUM implementation in C#
  + example of templates and placeholders
  + as with other similar works it may not be useful for us
- /Towards Generating Human-Oriented Summaries of Unit Test Cases/ ðŸŒŸ
  ([[http://www.cs.ubc.ca/~murphy/papers/summarization/icpc13era-t9-p-16545-preprint.pdf][kamimura2013towards]]) <<kamimura2013towards>>
- /Automated Documentation Inference to Explain Failed Tests/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.252&rep=rep1&type=pdf][zhang2011automated]]) <<zhang2011automated>>
  + could be used to improve the documentation and precision of ~try/catch~
    amplification
- /Automatically Identifying Focal Methods under Test in Unit Test Cases/
  ([[https://www.researchgate.net/profile/Mohammad_Ghafari3/publication/295918716_Automatically_Identifying_Focal_Methods_Under_Test_in_Unit_Test_Cases/links/57cd3d5f08ae89cd1e87bf9f.pdf][ghafari2015automatically]])
  + not useful, we are focusing on explaining edge cases

***** Commits/Code changes
- /On Automatically Generating Commit Messages via Summarization of Source Code Changes/
  ([[https://www.researchgate.net/profile/Luis_Cortes11/publication/267326224_On_Automatically_Generating_Commit_Messages_via_Summarization_of_Source_Code_Changes/links/5583f12208ae4738295bd3ca.pdf][cortes2014automatically]]) <<cortes2014automatically>>
  /ChangeScribe: A Tool for Automatically Generating Commit Messages/
  ([[http://www.cs.wm.edu/~denys/pubs/ICSE%2715-ChangeScribeTool-CRC.pdf][linares2015changescribe]])
  + Good entry point for the related work
  + Classifies commit with stereotypes
  + Uses templates for sentences, and fills it with commit stereotypes
    ([[dragan2011using]])
  + lacks 'why' information
- /Using Stereotypes to Help Characterize Commits/
  ([[http://www.cs.kent.edu/~jmaletic/papers/ICSM11.pdf][dragan2011using]]) <<dragan2011using>>
  + Only categorize based on added or deleted methods
- /Towards Automatic Generation of Short Summaries of Commits/
  ([[https://arxiv.org/pdf/1703.09603.pdf][jiang2017towards]])
- /Automatically Generating Commit Messages from Diffs using Neural Machine Translation/
  ([[https://arxiv.org/pdf/1708.09492.pdf][jiang2017automatically]])
  + trying to be less verbose and add context
- /On Automatic Summarization of What and Why Information in Source Code Changes/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551998][shen2016automatic]])
  + Better then /ChangeScribe/[[cortes2014automatically]]
  + Categories of Commits in Terms of Maintenance Task and Corresponding Description
    (based on [[swanson1976dimensions]]) (why information)
    +-----------------------+----------------------------------+
    | Categories of commits |           Description            |
    +-----------------------+----------------------------------+
    | Implementation        | New requirements                 |
    +-----------------------+----------------------------------+
    | Corrective            | Processing failure               |
    |                       | Performance failure              |
    |                       | Implementation failure           |
    +-----------------------+----------------------------------+
    | Adaptive              | Change in data environment       |
    +-----------------------+----------------------------------+
    | Perfective            | Processing inefficiency          |
    |                       | Performance enhancement          |
    |                       | Maintainability                  |
    +-----------------------+----------------------------------+
    | Non functional        | Code clean-up                    |
    |                       | Legal                            |
    |                       | Source control system management |
    +-----------------------+----------------------------------+
  + What information: description (more like diff (ChangeDistiller) dump) of
    changes
  + only keep information for methods that are called many times
  + boilerplates not interesting
- /Automatically Documenting Program Changes/
  ([[http://web.eecs.umich.edu/~weimerw/p/weimer-ase2010-deltadoc-preprint.pdf][buse2010automatically]]) <<buse2010automatically>>
  + precise description
  + nicely written, but not useful for us

***** General/Others
- /Comment Generation for Source Code: State of the Art, Challenges and Opportunities/
  ([[https://arxiv.org/pdf/1802.02971.pdf]])
  + TODO
  + Information Retrieval ("analyze the natural language clues in the source
    code") -> not relevant
  + Program Structure Information (summary from important statements) -> not
    relevant(?)
  + Software Artifacts Beyond Source Code (using the social interaction
    revolving around development) -> not relevant
  + Fundamental NLP Techniques -> not relevant
  + Not very useful... "current approach only generate descriptive comments"
- /The Emergent Laws of Method and Class Stereotypes in Object Oriented Software/
  ([[https://etd.ohiolink.edu/!etd.send_file?accession=kent1290570321&disposition=inline][dragan2011emergent]])
  + Excerpt from PhD Thesis
  + Source of the Taxonomy of Method Stereotypes ðŸŒŸ
  + C++
- /The Dimensions of Maintenance/
  ([[http://www.mit.jyu.fi/ope/kurssit/TIES462/Materiaalit/Swanson.pdf][swanson1976dimensions]]) <<swanson1976dimensions>>
  + Foundational paper
- /JStereoCode: Automatically Identifying Method and Class Stereotypes in Java Code/
  ([[https://dl.acm.org/citation.cfm?id=2351747][moreno2012jstereocode]])
  + Extending Dragan's work <<dragan2011using>> for Java
- /Automatic Documentation Inference for Exceptions/ ðŸŒŸ
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.8478&rep=rep1&type=pdf][buse2008automatic]]) <<buse2008automatic>>
  + well written
  + could be used to improve the documentation and precision of ~try/catch~
    amplification
- /Towards Automatically Generating Summary Comments for Java Methods/ ðŸŒŸ
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/200/towards-automatically-generating-summary-comments-for-methods.pdf?sequence=3][sridhara2010towards]]) <<sridhara2010towards>>
  (+ PhD thesis)
  - well written
  - SWUM, central lines selection, ...
  - again not exactly useful for us
- /Integrating Natural Language and Program Structure Information to Improve Software Search and Exploration/
  ([[https://search.proquest.com/openview/89d289c5561fc953875cf9d6f223a7cc/1?pq-origsite=gscholar&cbl=18750&diss=y][hill2010integrating]])
  + PhD thesis
  + Source of SWUM
  + SWUM implementation as Eclipse plugin
- /Swummary: Self-Documenting Code/
  ([[https://scholarscompass.vcu.edu/capstone/114/][herbert2016swummary]]) ([[https://github.com/herbertkb/Swummary][git]])
  + focal method extraction -> Swum.NET
- /Automatic Source Code Summarization of Context for Java Methods/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7181703][mcburney2016automatic]])
  + looks very complete but again not quite useful

**** Commits/Code survey
- /Whatâ€™s a Typical Commit? A Characterization of Open Source Software Repositories/
  ([[https://www.researchgate.net/profile/Huzefa_Kagdi/publication/4349695_What%27s_a_Typical_Commit_A_Characterization_of_Open_Source_Software_Repositories/links/00b7d528a6e2589336000000.pdf][alali2008s]])
  - Useful to know what terms to use
  - According to [[cortes2014automatically]] the most used terms are fix, add,
    test, bug, patch and the most used combinations are file-fix, fix-use,
    add-bug, remove-test, and file-update.
- /On the Nature of Commits/
  ([[https://sci-hub.tw/10.1109/ASEW.2008.4686322][hattori2008nature]])
- /What do large commits tell us? A taxonomical study of large commits/
  ([[http://maveric0.uwaterloo.ca/~migod/846/papers/msr08-hindle.pdf][hindle2008large]])
  + extending [[swanson1976dimensions]]
- /Cognitive Processes in Program Comprehension/
  ([[https://ac.els-cdn.com/016412128790032X/1-s2.0-016412128790032X-main.pdf?_tid=aff39f10-109e-11e8-8c6f-00000aacb360&acdnat=1518513618_e744f6cb72ebf42954fbb25e1eb42220][letovsky1987cognitive]])
  + Foundational paper
- /On the Naturalness of Software/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6227135][hindle2012naturalness]])
  + Code is repetitive and predictable

**** Natural Language Generator
- /SimpleNLG: A realisation engine for practical applications/
  ([[http://www.aclweb.org/anthology/W09-0613][gatt2009simplenlg]])
  + TODO

**** Code Evolution
- /Erlang Code Evolution Control/
  ([[https://arxiv.org/pdf/1709.05291.pdf][arXiv:1709.05291]])
  + TODO

**** Test Case Minimisation
- /Efficient Unit Test Case Minimization/
  ([[https://www.semanticscholar.org/paper/Efficient-unit-test-case-minimization-Leitner-Oriol/7ea90839a908f8a0b171d93fad72bcace2cdf0ad][leitner2007efficient]])
  + TODO
- /Yesterday, my program worked. Today, it does not. Why?/
  ([[https://dl.acm.org/citation.cfm?id=318946][zeller1999yesterday]])
  + TODO

**** Not Relevant
***** Knowledge
- /Poster: Construct Bug Knowledge Graph for Bug Resolution/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7965299][wang2017construct]])
- /Towards the Visualization of Usage and Decision Knowledge in Continuous Software Engineering/
  ([[https://wwwbruegge.in.tum.de/lehrstuhl_1/research/paper/johanssen2017visualization.pdf][johanssen2017towards]])
  + Pretty figures
  + Design of a tool to visualize various kinds of knowledge
- /Method Execution Reports: Generating Text and Visualization to Describe Program Behavior/
  ([[http://bergel.eu/MyPapers/Beck17a-MethodExecutionReports.pdf][beck2017method]])

***** Mutation Testing
- /Is Mutation Testing Ready to be Adopted Industry-Wide?/
  ([[https://www.researchgate.net/profile/Bruno_Rossi2/publication/309709540_Is_Mutation_Testing_Ready_to_Be_Adopted_Industry-Wide/links/59fb9709458515d07061a124/Is-Mutation-Testing-Ready-to-Be-Adopted-Industry-Wide.pdf][movzucha2016mutation]])
- /Investigating the Correlation between Mutation Score and Coverage Score/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6527442&tag=1][assylbekov2013investigating]])

***** Testing Related
- /SCOTCH: Test-to-Code Traceability using Slicing and Conceptual Coupling/
  ([[https://pdfs.semanticscholar.org/d38a/88ee65b56c2e3e3efc33c727d0990678683c.pdf][qusef2011scotch]])

***** Others
- /A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes/
  ([[https://arxiv.org/pdf/1704.04856.pdf][loyola2017neural]])
  + Multiple good citation to papers on NL and SE
- /Automatically Capturing Source Code Context of NL-Queries for Software Maintenance and Reuse/
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/199/Hill09.pdf?sequence=4][hill2009automatically]]) <<hill2009automatically>>
- /How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms/
  ([[https://dl.acm.org/citation.cfm?id=2486788.2486857][panichella2013effectively]])
  + Enhancement that doesn't really interest us
  + "in the context of three different SE tasks: (1) traceability link recovery,
    (2) feature location, and (3) software artifact labeling."
- /Software traceability with topic modeling/
  ([[https://dl.acm.org/citation.cfm?doid=1806799.1806817][asuncion2010software]])
  + "navigate the software architecture and view semantic topics associated with
    relevant artifacts and architectural components"
- /Automatically Detecting and Describing High Level Actions within Methods/
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/204/automatically-detetct-and-describe-high-level-actions-in-methods.pdf?sequence=1][sridhara2011automatically]]) <<sridhara2011automatically>>
  + too high level
- /Automatic Generation of Natural Language Summaries for Java Classes/
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/285/icpc13summaries-submitted.pdf?sequence=1][moreno2013automatic]])
- /Using Method Stereotype Distribution as a Signature Descriptor for Software Systems/
  ([[http://www.cs.kent.edu/~ndragan/ICSM09.pdf][dragan2009using]])
- /Reverse Engineering Method Stereotypes/
  ([[http://www.cs.kent.edu/~jmaletic/papers/ICSM06.pdf][dragan2006reverse]])
- /Supporting Program Comprehension with Source Code Summarization/
  ([[https://www.researchgate.net/profile/Jairo_Aponte/publication/215739380_Supporting_program_comprehension_with_source_code_summarization/links/554771110cf2e2031b36b7fd.pdf][haiduc2010supporting]])
  - motivations
- /Natural Language-based Software Analyses and Tools for Software Maintenance/
  ([[https://users.drew.edu/ehill1/papers/lncs12.pdf][pollock2009natural]])
  + more about analysis than generation

** Contribution
*** Minimisation
*** Focus
*** Replace original test or keep both
*** Explanation
**** Slicing
**** Natural Description
*** Ranking


* Development
[[https://github.com/STAMP-project/dspot/issues/187][Issue]]
** TODO Extract hard-coded amplifications messages
** TODO Automated PR
** TODO Identify which amplification is involved in which killed mutant


* Global Goals [0/2]
** TODO Report <2018-06-08 Fri 12:00>
- [X] Thanks all the team in report (Benjamin, Benoit, Martin)
** TODO Defense <2018-06-25 Mon>
*** DONE Talk @ Workshop Software Engineering Research <2018-03-08 Thu 10:00>--<2018-03-08 Thu 10:20>
- Room 4523
- 10 minutes talk
- [[https://docs.google.com/document/d/1NL3FGr_ruYRTY4824mHitkjwitKyBVcaddZLJpbtztA/edit]]
- /Mandatory/ slides [4/4]
  + [X] Problem statement
  + [X] Experiment protocol
  + [X] Experiment results
    - no results yet ðŸ˜ž
  + [X] Related works
*** TODO Talk @ Workshop Software Engineering Research <2018-05-10 Thu>
*** TODO Defense Rehearsal @ ENS <2018-06-22 Fri>


* Journal [2/21]
** DONE Preliminary Bibliographical Work <2017-09-18 Mon>--<2018-02-07 Wed>
*** Things Done
- Meeting with Benoit <2017-09-22 Fri>
  + [[https://github.com/STAMP-project/dspot/issues/187][1]], [[https://github.com/STAMP-project/dspot/issues/129][2]], [[https://github.com/STAMP-project/dspot/issues/54][3]] issues for possible work to do
  + 1 possible work: explain if a mutant isn't killed because of oracle or input
  + focus on mutation (e.g. mutation score)
  + work will be on [[https://github.com/STAMP-project/dspot][Dspot]] and [[https://github.com/STAMP-project/pitest-descartes][PIT]].
- Read [[http://massol.myxwiki.org/xwiki/bin/view/Blog/MutationTestingDescartes][blog on PIT and Descartes]]
  + Sum up PIT/Descartes
  + List of wanted features
- Meeting with Benoit <2017-11-23 Thu>
  + The purpose of DSpot has shifted right?
    - interesting to talk about the history in bibliography? No, there is a new
      paper
  + Enough space to talk about related work? present a few papers in details and
    cite others
  + Current organisation of bibliography
    - General techniques
      + Definitions
      + Mutants
      + etc
    - Useful tools
      + DSpot
  + do extensive evaluation (comparison from scratch vs amplification)
  + find literals to help tests
  + add mutation operator for specific data structures
  + stack mutations
  + add explanations
  + 3 big open problems
- Meeting with Benoit <2017-12-22 Fri>
  + reduce only the generated tests
  + big question: minimal generated tests
    - pre or post treatement
    - order of presenting PRs
    - this is the big question
    - we don't want to touch the original suite
    - we want the programmer to understand the new tests
  + add an example of junit test
  + talk about the trend of genetic improvement
  + don't necesseraly cite /Automatic software diversity in the light of test
    suites/ and /Tailored source code transformations to synthesize
    computationally diverse program variants/
- Talk rehearsal <2018-01-28 Mon 08:30>, notes by Vladislav
  - More illustrations (workflow graph?)
  + Check the test case example (too complicated for not much, not really java)
  + Year and conference acronym in footcite
  + Careful with lambdas for TDR (check with supervisor)
  + More details on commits/pull requests and emphasize the importance of
    developers reviewing generated tests
  + Slide 10 -> ugly (different spacings)
  + Stacking operators: explanation too sparse
  + 4th point in conclusion slide too vague. Not just the goal but also the mean
    to achieve it
- [[https://blog.acolyer.org/2018/01/23/why-is-random-testing-effective-for-partition-tolerance-bugs/]]

*** Blocking Points

*** Planned Work [6/6]
- [X] Read papers
- [X] Meeting with Benoit <2017-09-22 Fri 15:00-15:30>
- [X] Meeting with Benoit <2017-11-23 Thu 15:00-16:00>
- [X] Send link to repo
- [X] Ask Maud about plane tickets refund
- [X] Meeting with Benoit <2017-12-22 Fri 10:30-11:30>


** DONE Week 1 & 2 <2018-02-07 Wed>--<2018-02-18 Sun>
*** Things Done
- Wrote the little example of use of Spoon (I simply added it in [[https://github.com/SpoonLabs/spoon-examples][spoon-examples]])
#+NAME: RemoveIf
#+BEGIN_SRC java
package fr.inria.gforge.spoon.transformation;

import spoon.processing.AbstractProcessor;
import spoon.reflect.code.*;

/**
 * Removes if when there is no else and if the body consists only of a return
 *
 * @author Simon Bihel
 */
public class RemoveIfReturn extends AbstractProcessor<CtIf> {

    @Override
    public void process(CtIf element) {
        CtStatement elseStmt = element.getElseStatement();
        if (elseStmt == null) { return; } // should not be an else

        CtStatement thenStmt = element.getThenStatement();
        if (thenStmt instanceof CtReturn) { // simple case with directly a then statement
            element.replace(thenStmt);
            return;
        }
        if (thenStmt instanceof CtBlock) { // case with a block which first statement is a return
            CtStatement firstStmt = ((CtBlock) thenStmt).getStatement(0);
            if (firstStmt instanceof CtReturn) {
                element.replace(thenStmt);
            }
        }
    }
}
#+END_SRC
#+Name: RemoveIfTest
#+BEGIN_SRC java
#+END_SRC
- [[https://clang-analyzer.llvm.org/][Clang static analyzer]] for windows
  + Clang is painful to install on Windows... It requires llvm and Microsoft
    Visual Studio. And there is no other choice than building from source. And
    it requires Perl to run.
  + Should probably use [[http://cppcheck.sourceforge.net/][CPPcheck]]
  + Cppcheck has a GUI and an installer for Windows. ðŸ‘
  + example of bugs [[http://courses.cs.vt.edu/~cs1206/Fall00/bugs_CAS.html]]
  + no bug in the provided code
- Software Maintenance seems to be an important keyword/field for the
  documentation of code
- To what extent are documenting source code changes useful for us?
  + Only few changes made by DSpot
  + The source of the change is a tool, not a human
  + Still useful to see how they formulate features in natural language
  + DSpot doesn't add new features, we want the purpose of enhanced tests.
  + Don't really care about Pyramid method because it compares with human
    written messages
- GitHub's [[https://help.github.com/articles/creating-a-pull-request-template-for-your-repository/][PR templates]] are just plain text templates.
- Went through papers that cited ChangeScribe. Went partly through citations by
  ChangeScribe.
- Spent a lot of time on generating natural language from source code
- Submitted a [[https://github.com/jceb/vim-orgmode/pull/291][fix]] for a bug in vim-orgmode
- Natural Language Generators
  + found on github, for java
    1. [[https://github.com/simplenlg/simplenlg][SimpleNLG]]
      - 410 stars, 215 citations
      - Seems to be just what we need
    2. [[https://github.com/kariminf/nalangen][NaLanGen]]
      - 2 stars
  + ChangeScribe seems to use a homemade generator
- "The Software Word Usage Model (SWUM) is one of the first models of this type,
  and can be used for converting Java method calls into natural language
  statements (Hill et al., 2009)."
- Looking at the code of DSpot to get info on generated tests
  + looks like a list of amplified test are generated and you don't know what
    was the amplifier

*** Blocking Points
- Is it useful to explore approaches for augmenting the context provided by
  differencing tools?

*** Planned Work [6/12]
- [X] Read papers
- [ ] should I register for ICST? and +ICSE+? -> Yes, talk/remind Benoit
  - Not eligible for [[http://www.es.mdh.se/icst2018/kaist-diversity-student-travel-awards/]]
- [X] Sign papers grant
- [X] Is there a Slack or something?
- [-] Get familiar with Spoon
  + [ ] Read paper
  + [-] Little project, remove ~if~ when there is no ~else~ and the body is
    just a ~return~.
    - [X] Write the program
    - [ ] Write tests
- [ ] Get familiar with Dspot
  + [ ] Running it
  + [ ] Contributing
    - [ ] Pick issues
    - [ ] Fix them
- [-] See /boiler-plates/ for NLP way of building sentences.
  + a.k.a templates, placeholder templates
  + [ ] Search for papers and read them
  + [X] Search for tools
- [X] Sign contract with KTH Relocation <2018-02-13 Tue 14:00>--<2018-02-13 Tue 15:30>
- [X] Categorize papers of preliminaries
- [X] Lookup what static analysis is possible with +clang+ Cppcheck [100%]
  + [X] find tools
  + it is for mechatronics students who write small programs for arduinos
  + show them what tests are and what's possible to discover bugs
  + [X] Think of what they could be taught
  + [X] Test Cppcheck on a windows machine
    - [X] Install windows on the small computer
    - [X] Test the code provided in the course
- [ ] Go to EntrÃ© for badge and PIN code outside working hours
- [ ] Run tools that I encounter in papers


** DONE Week 3 <2018-02-19 Mon>--<2018-02-25 Sun>
*** Things Done
- Work on DSpot documentation
- Read reviews of bibliographic report
- How to remember what amplification has been applied?
  + +Go through logs+
    - nothing useful in them
  + Comments directly in the code
    - name of the amplifier used in the line before
    - could easily be enriched if necessary
  + +Enrich test methods with a new parameter+
    - last resort
- A =json= file summarizes killed mutants (with their location)
- Need to keep focus
#+BEGIN_QUOTE
To select the new test case to be proposed as pull request, we look for an
amplified test that kills mutants which are all located in the same method.
#+END_QUOTE
(this was done manually)
- Need for automated minimization
#+BEGIN_QUOTE
A second point in the preparation of the pull request relates to the length of
the amplified test. Once a test method has been selected as a candidate pull
request, we analyze it and manually make it clearer and more concise, we call
this process the manual minimization of the amplified test. We note that
automated min- imization of amplified tests is an interesting area of future
work, left out of the scope of this paper.
#+END_QUOTE
- SWUM is really about analysis. Trying to reformulate things without making
  sense of them.
- Possible title: Adaptation of Amplified Unit Tests for Human Comprehension
- [[https://github.com/abb-iss/Swum.NET][Swum.NET]]
#+BEGIN_QUOTE
UnitTestScribe also uses SWUM.NET to generate a general NL description for each
unit test case method. SWUM.NET captures both linguistic and structural
information about a program, and then generates a sentence describing the
purpose of a source code method.
#+END_QUOTE
- Started writing
- Made a [[https://github.com/rhysd/vim-grammarous/pull/59][PR]] for vim-grammarous
- [[https://github.com/STAMP-project/dspot/issues/54][Discussion]] on how to minimize generated tests

*** Blocking Points
- [X] Where is the `keep test that kills mutants all located in the same
  method'? Seems to be implemented reading the paper, but [[https://github.com/STAMP-project/dspot/issues/130][issue]] still open and
  it proposes a solution that seems different than just looking at the json file
  at then end of the process.
  + it was done manually

*** Planned Work [7/12]
- [X] Read papers
- [ ] Register for ICST
- [-] +Get familiar with+ Dspot [1/6]
  + [X] Running it
  + [ ] Contributing
    - [ ] Pick issues
    - [ ] Fix them
  + [-] Write documentation [2/4]
    - [-] Key methods [3/5]
      + [X] Assertion generation [2/2]
        - [X] ~AssertGenerator~
        - [X] ~MethodsAssertGenerator~
      + [-] Input amplification [1/2]
        - [X] glue
        - [ ] amplifiers
      + [X] Pre-amplification
      + [X] Amplification
      + [-] Compilation & run [2/3]
        - [X] ~compileAndRunTests~
        - [X] ~buildClassForSelection~
        - [ ] ~TestCompiler~
    - [X] Rename ~amplifyTests~ to express the fact that it is only doing input
      amplification
    - [ ] ~compileAndRunTests~
      + [ ] Why return ~null~ when not all methods were compilable or some
        tests failed?
    - [X] Renaming plural variables
  + [ ] Work on removing all deprecated classes in stamp [0/1]
    - [ ] Remove unused deprecated methods of ~TestSelector~
  + [ ] More precise ~try/catch~?
    - Would that be useful? Feasible?
  + [ ] Extract hard-coded amplifications messages
- [X] Lab access denied outside working hours
  + [X] Go to EntrÃ©
  + [X] Go again to EntrÃ©
  + [X] Send email to request access to the lab
    - resend
  + [X] Resolved
- [X] Run tools that I encounter in papers
  + tools not really useful are they(?)
  + closing this for now
- [X] Find a way to know which amplifications have been applied and/or how to
  implement it
- [X] Make DHELL [[https://github.com/STAMP-project/dhell/pull/3][PR]] maven compiler version
  + [[https://github.com/spring-guides/gs-maven/issues/21]]
- [-] Start writing [0/4]
  + [-] Problem statement
    - [X] scientific
      + quite short
    - [ ] technical
  + [-] Comparison with works on description
    - [X] Explaining what they do
      + badly written
      + quite short
    - [ ] Why we can't apply them for our work
  + [ ] Comparison with works on test cases minimization
    - [ ] Explaining what they do
    - [ ] Why we can't apply them for our work
  + [ ] Whether using an NLG is useful
- [X] +Start doing a simple NL commit messages generator+
  + for later, first we need minimization
- [X] Maybe reorganize the references on descriptions
- [ ] Read about identify essential parts of a test for killing a specific
  mutant
- [ ] Search for papers on mutation testing and same location targeting


** DONE Week 4 <2018-02-26 Mon>--<2018-03-04 Sun>
*** Things Done
- Added git hook to commit the html version of the reporting
- Explored the use of slicing to detect the cause of new killed mutant
  + Need observation-based slicing with mutation score(?)
- Nothing on summarization and mutation testing
  + You usually think the other way around, what do I need to do in order to
    kill this new mutant
- [[https://github.com/srcML/srcSlice][srcSlice]] not supporting Java ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7883355&tag=1][paper]])
- [[https://github.com/hammacher/javaslicer][JavaSlice]] does not support Java 8
- [[http://indus.projects.cs.ksu.edu/projects/kaveri.shtml][Kaveri]] (Indus Java Program Slicer) old and eclipse plugin
- [[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8080067<Paste>][JavaBST]] not available ? paper badly written
- [[http://wala.sourceforge.net/wiki/index.php/Main_Page][WALA]]
- Fixed org export and also pull on server
- Starred every vim plugin I use with Github's API and [[https://github.com/PyGithub/PyGithub][PyGitbub]]
- Explored end-user description of Pitest mutators
  + Pitest has user-friendly mutators, now the question is how to use/access
    them
-
#+BEGIN_SRC sh
cd .. && mvn clean package -DskipTests && cd dhell && mvn clean package && java -jar ../dspot/target/dspot-1.0.6-SNAPSHOT-jar-with-dependencies.jar -p ./dspot.properties -i 1 -t eu.stamp.examples.dhell.HelloAppTest -a MethodAdd --verbose && vim dspot-out/eu.stamp.examples.dhell.HelloAppTest_mutants_killed.json
#+END_SRC

*** Blocking Points
- NL commit message generator
  + how to know which amplifications were applied?
- What is a program/test slice for a mutation score criterion?
  + dataflow slice starting from the killing assertion

*** Planned Work [3/9]
- [ ] Register for ICST
- [-] Dspot [2/5]
  + [X] Contributing
  + [X] Write documentation [2/2]
    - [X] Key methods [2/2]
      + [X] Input amplification
        - [X] amplifiers
      + [X] Compilation & run
        - [X] ~TestCompiler~
          + no need
    - [X] ~compileAndRunTests~
      + [X] Why return ~null~ when not all methods were compilable or some
        tests failed?
        - Created an [[https://github.com/STAMP-project/dspot/issues/336][issue]]
    - [[https://github.com/STAMP-project/dspot/pull/337][PR]]
  + [ ] Work on removing all deprecated classes in stamp
    - [ ] Remove unused deprecated methods of ~TestSelector~
  + [ ] More precise ~try/catch~?
    - Would that be useful? Feasible?
  + [ ] Extract hard-coded amplifications messages
- [ ] Start writing [0/4]
  + [ ] Problem statement
    - [ ] technical
  + [ ] Comparison with works on description
    - [ ] Why we can't apply them for our work
  + [ ] Comparison with works on test cases minimization
    - [ ] Explaining what they do
    - [ ] Why we can't apply them for our work
  + [ ] Whether using an NLG is useful
- [X] Read about identify essential parts of a test for killing a specific
  mutant
- [X] Search for papers on mutation testing and same location targeting
- [-] Start doing a simple NL commit messages generator [0/2]
  - [ ] DSpot automated PR
  - [-] Simple PR description [3/4]
    + [X] Add a field in the killed mutants ~json~ file
    + [X] Print it
      - done automatically
    + [X] Stupid message
    + [ ] Long stupid description
      - [ ] Get what amplifications were applied
      - [ ] done
- [X] Replace ~fr.inria.stamp~ with ~eu.stamp~
  + [[https://github.com/STAMP-project/dspot/pull/339][PR]]
- [ ] Classification of mutators
- [-] Integrate WALA to compute a slice per new mutant [1/4]
  + [X] Need a more precise location for the mutant location
    - +column number+
      + not available
    - maybe I don't need it
  + [ ] Need to know the killing assertion
    - [ ] Add a trace of this when a test is kept
  + [ ] Adding as dependency
  + [ ] Use it


** TODO Week 5  <2018-03-05 Mon>--<2018-03-11 Sun>
*** Things Done
- Tried to use Sourcetrail
  + Needed to run ~mvn install -DskipTests -Djacoco.skip=true~
  + displayed no references or class
- Worked on presentation for the workshop
- Proposed mutators taxonomy
  + Literal change
  + Object change
  + New assertion
- Meeting with Benoit
  + in commit message, talk about bugs instead of mutants
  + 3 steps
    - oracle enhancement only
    - new input
    - combination
  + write why the problem is difficult
  + write different kinds of message with each a specific focus
  + maybe compare trace of amplified test vs original
  + study commit messages related to tests
- ~git log --grep "^Tested"~

*** Blocking Points
- [ ] What will the *scientific* contribution be?
- [ ] What kind of evaluation?

*** Planned Work [2/10]
- [X] Talk @ Workshop Software Engineering Research <2018-03-08 Thu 10:00>--<2018-03-08 Thu 10:20>
  + Workshop <2018-03-08 Thu 09:30>--<2018-03-08 Thu 12:30>
  + Room 4523
- [ ] Register for ICST
- [ ] Dspot [0/1]
  + [ ] Extract hard-coded amplifications messages
- [-] Start writing [3/4]
  + [X] Problem statement
    - [X] technical
  + [X] Comparison with works on description
    - [X] Why we can't apply them for our work
  + [X] Comparison with works on test cases minimization
    - [X] +Explaining what they do+
      + rephrase a description from a survey or something
    - [X] Why we can't apply them for our work
  + [ ] Whether using an NLG is useful
- [ ] Start doing a simple NL commit messages generator
  + [ ] Long stupid description
    - [ ] Get what amplifications were applied
    - [ ] done
- [ ] Classification of mutators
- [-] Integrate WALA to compute a slice per new mutant [1/4]
  + [X] Need a more precise location for the mutant location
    - +column number+
      + not available
    - maybe I don't need it
  + [ ] Need to know the killing assertion
    - [ ] Add a trace of this when a test is kept
  + [ ] Adding as dependency
  + [ ] Use it
- [X] Fix [[https://github.com/STAMP-project/dspot/issues/336]]
  + [[https://github.com/STAMP-project/dspot/pull/350][PR]]
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing




** TODO Week 6  <2018-03-12 Mon>--<2018-03-18 Sun>
*** Things Done

*** Blocking Points

*** Planned Work [0/1]
- [ ] Change apartment <2018-03-15 Thu>


** TODO Week 7  <2018-03-19 Mon>--<2018-03-25 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 8  <2018-03-26 Mon>--<2018-04-01 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 9  <2018-04-02 Mon>--<2018-04-08 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 10 <2018-04-09 Mon>--<2018-04-15 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 11 <2018-04-16 Mon>--<2018-04-22 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 12 <2018-04-23 Mon>--<2018-04-29 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 13 <2018-04-30 Mon>--<2018-05-06 Sun>
*** Things Done

*** Blocking Points

*** Planned Work [0/1]
- [ ] Renew SL access card


** TODO Week 14 <2018-05-07 Mon>--<2018-05-13 Sun>
*** Things Done

*** Blocking Points

*** Planned Work [0/1]
- [ ] Workshop <2018-05-10 Thu>


** TODO Week 15 <2018-05-14 Mon>--<2018-05-20 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 16 <2018-05-21 Mon>--<2018-05-27 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 17 <2018-05-28 Mon>--<2018-06-03 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 18 <2018-06-04 Mon>--<2018-06-10 Sun>
*** Things Done

*** Blocking Points

*** Planned Work [0/1]
- [ ] Report <2018-06-08 Fri 12:00>


** TODO Week 19 <2018-06-11 Mon>--<2018-06-17 Sun>
*** Things Done

*** Blocking Points

*** Planned Work


** TODO Week 20 <2018-06-18 Mon>--<2018-06-24 Sun>
*** Things Done

*** Blocking Points

*** Planned Work [0/1]
- [ ] Defense Rehearsal @ ENS <2018-06-22 Fri>


** TODO Week 21 <2018-06-25 Mon>--<2018-07-01 Sun>
*** Things Done

*** Blocking Points

*** Planned Work [0/1]
- [ ] Defense <2018-06-25 Mon>


]]* Conclusion
