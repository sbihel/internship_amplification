# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer hidestars entitiespretty
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
# #+INFOJS_OPT: view:info toc:nil

#+TITLE: Internship with Benoit Baudry at KTH
#+SUBTITLE: Adaptation of Amplified Unit Tests for Human Comprehension
#+DATE: <2018-02-07 Wed>--<2018-06-22 Fri>
#+AUTHOR: Simon Bihel
#+EMAIL: [[mailto:simon.bihel@ens-rennes.fr]]
#+WEBSITE: [[simonbihel.me]]
#+LINK: [[https://github.com/sbihel/internship_amplification]]
#+LANGUAGE: en
#+KEYWORDS: labbook, internship


* Introduction


* Findings
** Bibliography
*** Writing
**** Context

**** Problem statement
Amplified tests have many new assertions and new inputs and DSpot keeps tests
depending on the fact that they detect new mutants. What that means is that
there is no enquiry on the usefulness of each amplification. Because the number
of new mutants killed is often times significantly lower than the number of
amplifications --- especially the number of assertions which are the one to
effectively detect a mutant --- we end up with a lot of useless statements. This
noise is problematic because it makes the review process longer, and because the
less focused a program is, the harder it is to comprehend. And noise is not the
only threat to focus as the new mutants can be completely different and from a
different part of the SUT compared to the ones the original test case could
detect. The final step towards a human-friendly output would be to add context
and human-like description of the amplification. To sum-up, the three identified
problems to tackle are noise reduction, focus confinement, and natural language
description of the amplification.

**** Technical Difficulties
In order to describe a mutant, you need information about it. You could give the
mutator "category" but you only have its class name. You could give the column
to the statement it was applied to to highlight it, maybe, but you do not have
access to that. You could use the position in the AST, but you do not know that.

Knowing the which assertion killed what mutant is essential. Be it to start a
program slice from that assertion, or simply paraphrasing the assertion to
explain what bug is detected. But you do not know that. And you barely know, with
ugly comments, what assertions are the result of amplifications. Identifying,
afterwards, the role that assertions play is cumbersome. You can run the test
case with every mutants. But first, you do not have directly access to mutants.
And what do you want to do with them? Instrument, by adding a probe after each
assertion? How do you automate elegantly such stream of test execution? Maybe
you can remove assertions, one by one, and see if mutants keep getting killed? I
know we are in SBSE but that is quite ugly.

As said before we have no direct information on the position of amplifications
in the new test case. Makes it harder to generate descriptions or apply
minimization on them. But what data structure would you use? Bookkeeping of the
position in the AST? How would you keep it up-to-date with multiple rounds of
amplifications?

**** On the usefulness of works from (code maintenance|software artifacts summarization)
A lot of effort has been put in generating human friendly descriptions for
various kinds of software artifacts. In particular, there have been works on
generating documentation (or comments or description or summary) for source
code, methods, and more interestingly unit test cases. These tools can generate
natural language description of /what/ the piece of code does and identify to
most important parts or lines of code. But as for /why/ a code change was done
or the role a piece of code plays --- i.e. understand the intentions of the
developers --- it is harder and tools need additional information or limit the
scope by identifying stereotypes (e.g. labelling a commit as Feature Addition).

But those works are not directly useful for our problem. First, we know why an
amplified test was kept, it is because it can detect a new bug.

**** On the usefulness of works from test cases minimization
Using delta-diff we can identify useless statement and then remove them. But
more powerful program minimisation tools are available. While we could predict
that the more minimisation is applied, the less code there is left to describe
thus the description is easier to generate, it is not obvious and others details
have to be taken into account.

First we might not want to modify the original part, as the developer might
already be familiar with it and it might be less overwhelming to grasp the
purpose of the test case. And even if the developer has never seen this test
before, a hand-written program is probably easier to understand than a compact
version.

And tools probably cannot be told not to touch certain parts.

**** On the usefulness of an NLG
The sentences should always be the same, follow the same structure, built with
the same template as humans do.

**** What do we then propose as contribution


*** References
**** Cultural
- /Search Based Software Engineering: Techniques, Taxonomy, Tutorial/
  ([[https://www.researchgate.net/profile/Mark_Harman/publication/221051156_Search_Based_Software_Engineering_Techniques_Taxonomy_Tutorial/links/0046352052592d5c2c000000/Search-Based-Software-Engineering-Techniques-Taxonomy-Tutorial.pdf][harman2012search]])
  + TODO
- /A Few Billion Lines of Code Later/
  ([[https://pdfs.semanticscholar.org/295f/4ffa651675b22ae8e2f3f30b400330da0c69.pdf][bessey2010few]])
  + Great to understand the limits of static analysis but also some of the
    limits of all analysis
  + Difficult to analyze code because of the diversity of build automation tools
  + "By default, companies refuse to let an external force modify anything."
  + "A misunderstood explanation means the error is ignored or, worse,
    transmuted into a false positive."
  + Many standards
  + Some people don't care about bugs, sometimes improving the tool reveals more
    bugs which is bad for the manager
- /Lessons from Building Static Analysis Tools at Google/
  ([[https://dl.acm.org/citation.cfm?id=3188720][sadowski2018lessons]])
  /Tricorder: Building a Program Analysis Ecosystem/
  ([[https://dl.acm.org/citation.cfm?id=2818828][sadowski2015tricorder]])
  + Great to understand the challenges in pushing an analysis tool in the real
    world
  + notes on the printed paper
  + such tool need to be
    1. Integrated/Easy to use
    2. Free of false positive
    3. Easy to understand
- /Spoon: A Library for Implementing Analyses and Transformations of Java Source Code/
  ([[https://hal.archives-ouvertes.fr/hal-01078532v2/document][pawlak2016spoon]])
  + let's say it's like llvm/clang
- /Regression Testing Minimisation, Selection and Prioritisation : A Survey/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.169.8696&rep=rep1&type=pdf][yoo2012regression]])
  + TODO
- /Clustering Test Cases to Achieve Effective & Scalable Prioritisation Incorporating Expert Knowledge/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.211.9479&rep=rep1&type=pdf][yoo2009clustering]])
  + TODO
- /Measuring software redundancy/
  ([[https://pdfs.semanticscholar.org/0a93/144638ebfc924550798b620835a3fc9785cf.pdf][carzaniga2015measuring]]) <<carzaniga2015measuring>>
  + TODO
- /Automatic Software Diversity in the Light of Test Suites/
  ([[https://arxiv.org/pdf/1509.00144.pdf][baudry2015automatic]])
  + analysis of common features (e.g. number of tests covering one statement)
  + plastic behavior (have different behaviors while still remaining correct)
    study
  + different details compared to [[baudry2015dspot]] and [[baudry2014tailored]]
- /Tailored source code transformations to synthesize computationally diverse program variants/
  ([[https://arxiv.org/pdf/1401.7635][baudry2014tailored]]) <<baudry2014tailored>>
  + More details than in [[baudry2015dspot]]
- /Selecting a Software Engineering Tool: Lessons Learnt from Mutation Analysis/
  ([[http://onlinelibrary.wiley.com/doi/10.1002/spe.2312/epdf][delahaye2015selecting]]) <<delahaye2015selecting>>
  + TODO
- /The Oracle Problem in Software Testing: A Survey/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6963470][barr2015oracle]])
  + TODO
- /Human-Centered Design Meets Cognitive Load Theory: Designing Interfaces that Help People Think/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.501.6467&rep=rep1&type=pdf][oviatt2006human]])
- /Grounded Theory in Software Engineering Research: A Critical Review and Guidelines/
  ([[https://ulir.ul.ie/bitstream/handle/10344/5396/Stol_2016_grounded.pdf?sequence=2][stol2016grounded]])
- /Five Misunderstandings about Case-study Research/
  ([[http://journals.sagepub.com/doi/pdf/10.1177/1077800405284363][flyvbjerg2006five]])
  + TODO
- /Is Search-based Unit Test Generation Research Stuck in a Local Optimum?/ 🌟🌟
  ([[https://dl.acm.org/citation.cfm?id=3105439][rojas2017search]])
  + list of challenges
    1. Searching Flat Fitness Landscapes
    2. Achieving High Code Coverage
    3. Tests Without Oracles
    4. Ugly Generated Test Code
    5. Research Papers Instead of Usable Tools

**** Unit Testing
- /A Survey of Unit Testing Practices/
  ([[http://ansymore.uantwerpen.be/system/files/uploads/courses/SE3BAC/p_05_01RunesonUnitTestPractices.pdf][runeson2006survey]])
  + TODO
- /WAP: Cognitive aspects in unit testing: The hunting game and the hunter's perspective/ 🌟🌟
  ([[http://chisel.cs.uvic.ca/pubs/prado-ISSRE2015.pdf][prado2015wap]])
  + TODO
  + 1st in Prado's trilogy
- /Advances in the Characterization of Cognitive Support for Unit Testing: The Bug-Hunting Game and the Visualization Arsenal/ 🌟🌟
  ([[http://ieeexplore.ieee.org/abstract/document/7789403/][prado2016advances]])
  + TODO
  + 2nd in Prado's trilogy
- /Towards cognitive support for unit testing: a qualitative study with practitioners/ 🌟🌟🌟
  ([[https://www.sciencedirect.com/science/article/pii/S0164121218300529][prado2018towards]])
  + 3rd in Prado's trilogy
  + nicely written
  + why didn't I hear about this research before...
  + 2 previous works already by Prado
  +
  + cites google blog post on flaky tests
    - [[https://testing.googleblog.com/2016/05/flaky-tests-at-google-and-how-we.html]]
  + Jasmine (testing framework for javascript includes directly NL description)
    - [[https://jasmine.github.io/2.0/introduction.html]]
  + TODO
  + the internship should have been centred around it
- /A Survey on Unit Testing Practices and Problems/
  ([[https://pdfs.semanticscholar.org/f193/e68ce6f3200b1f801e64bf49e56f668fd3ef.pdf][daka2014survey]])
  + TODO
- /The Impact of Test Case Summaries on Bug Fixing Performance: An Empirical Investigation/
  ([[http://ieeexplore.ieee.org/abstract/document/7886933/][panichella2016impact]])
  + TODO

**** Mutation Testing
- /Is Mutation Testing Ready to be Adopted Industry-Wide?/
  ([[https://www.researchgate.net/profile/Bruno_Rossi2/publication/309709540_Is_Mutation_Testing_Ready_to_Be_Adopted_Industry-Wide/links/59fb9709458515d07061a124/Is-Mutation-Testing-Ready-to-Be-Adopted-Industry-Wide.pdf][movzucha2016mutation]])
- /Investigating the Correlation between Mutation Score and Coverage Score/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6527442&tag=1][assylbekov2013investigating]])
- /An Analysis and Survey of the Development of Mutation Testing/ 🌟
  ([[https://pdfs.semanticscholar.org/3277/8a2eb4c74cd437e922ac1eb6a1477dfcb925.pdf][jia2011analysis]])
  + TODO
- /PIT: A Practical Mutation Testing Tool for Java (Demo)/ 🌟
  ([[https://dl.acm.org/citation.cfm?id=2948707][coles2016pit]])
  + Well written
  + PIT is fast (manipulate bytecode), which is one of the reasons it can be
    used in real life
  + test selection
  + robust, easy to use, well integrated (cites [[delahaye2015selecting]])
- /Resolving the Equivalent Mutant Problem in the Presence of Non-determinism and Coincidental Correctness/
  ([[https://pdfs.semanticscholar.org/eab1/f4c6259b0adc3a65ecd563380e5375a54e96.pdf][patel2016resolving]])
  + TODO
- /An Experimental Evaluation of PIT’s Mutation Operators/
  ([[http://www.diva-portal.org/smash/get/diva2:1161760/FULLTEXT01.pdf][andersson2017experimental]])
  + TODO
- /Are Mutation Scores Correlated with Real Fault Detection?/
  ([[http://orbilu.uni.lu/bitstream/10993/34950/1/ICSE-main18b%20%281%29.pdf][papadakis2018mutation]])
  + TODO
- /A Transformational Language for Mutant Description/
  ([[https://www.sciencedirect.com/science/article/pii/S1477842408000420][simao2009transformational]])
  + TODO
  + unfortunately it doesn't give clues on how to describe mutants as they see
    mutation simply as a match-and-replace process.
  + kind of look like a formal description of the design of a DSL
- /An Experimental Evaluation of Selective Mutation/
  ([[https://dl.acm.org/citation.cfm?id=257597][offutt1993experimental]])
- /A theoretical study of fault coupling/
  ([[https://onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1099-1689(200003)10:1%3C3::AID-STVR196%3E3.0.CO;2-P][wah2000theoretical]])
- /Proteum IM 2.0: An Integrated Mutation Testing Environment/ 🌟🌟
  ([[https://link.springer.com/chapter/10.1007/978-1-4757-5939-6_17][delamaro2001proteum]])
  + TODO

**** Search-based Software Testing
- /Search-based software testing: Past, present and future/
  ([[http://mcminn.io/publications/c18.pdf][mcminn2011search]])
  + Already read from previous internship
- /Genetic Improvement of Software: a Comprehensive Survey/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7911210][petke2017genetic]])
  + TODO
  + [[http://www.cs.bham.ac.uk/~wbl/biblio/][http://www.cs.bham.ac.uk/~wbl/biblio/]]
- /Evosuite/ 🌟
  ([[http://www.evosuite.org/evosuite/][fraser2011evosuite]]) ([[https://pdfs.semanticscholar.org/df36/d5c8c8ecace7f5b9347a0880daf2c10b3d4b.pdf][fraser2013evosuite]])
  + State-of-the-art tool
  + Very sophisticated, difficult to reproduce experiments because it changes
    fast and a lot of parameters are tweaked
  + minimization
    - remove unnecessary statements
    - careful not to generate long test cases
- /An Approach to Test Data Generation for Killing Multiple Mutants/ 🌟
  ([[http://ieeexplore.ieee.org/abstract/document/4021328/][liu2006approach]])

**** Test Amplification
- /B-Refactoring: Automatic Test Code Refactoring to Improve Dynamic Analysis/
  ([[https://hal.archives-ouvertes.fr/hal-01309004/file/banana-refactoring.pdf][xuan2016b]])
  + Split tests for each fragment to cover a simple part of the control flow.
  + Help with respect to fault localization.
- /Test data regeneration: generating new test data from existing test data/
  ([[http://www0.cs.ucl.ac.uk/staff/mharman/stvr-regeneration.pdf][yoo2012test]]) <<yoo2012test>>
- /The Emerging Field of Test Amplification: A Survey/
  ([[https://arxiv.org/pdf/1705.10692.pdf][danglot2017emerging]])
  + Dense
  + Good overview of goals (Table 1) and methods (Table 2)
- /DSpot: Test Amplification for Automatic Assessment of Computational Diversity/
  ([[https://arxiv.org/pdf/1503.05807.pdf][baudry2015dspot]]) <<baudry2015dspot>>
  + Comparison with TDR [[yoo2012test]] and also concurrent to
    [[carzaniga2015measuring]]
    - "the key differences between DSpot and TDR are: TDR stacks multiple
      transformations together; DSpot has more new transformation operators on
      test cases: DSpot considers a richer observation space based on arbitrary
      data types and sequences of method calls."
    - "We count the number of variants that are identified as computationally
      different using DSpot and TDR. "
- /A Systematic Literature Review on Test Amplification/ 🌟
  + TODO
- /Genetic-Improvement based Unit Test Amplification for Java/ 🌟
  + TODO
- /Dynamic Analysis can be Improved with Automatic Test Suite Refactoring/
  ([[https://arxiv.org/pdf/1506.01883.pdf][xuan2015dynamic]])
  + TODO
- /Automatic Test Case Optimization: A Bacteriologic Algorithm/
  ([[https://www.researchgate.net/profile/Jean-Marc_Jezequel/publication/3248230_Automatic_Test_Case_Optimization_A_Bacteriologic_Algorithm/links/0912f50ca4c15eb416000000.pdf][baudry2005automatic]])
  + TODO
  + Compared to DSpot, no assertions generation, small programs.

**** Automated Test Generation
- /How Do Automatically Generated Unit Tests Influence Software Maintenance?/ 🌟🌟
  ([[http://conferences.computer.org/icst/2018/#!/toc/0][shamshiri2018how]])
  + TODO
- /Generating Unit Tests with Descriptive Names Or: Would You Name Your Children Thing1 and Thing2?/ 🌟🌟🌟
  ([[https://dl.acm.org/citation.cfm?id=3092727][daka2017generating]])
  + TODO
- /An Empirical Investigation on the Readability of Manual and Generated Test Cases/ 🌟🌟🌟
  ([[https://giograno.me/assets/pdf/conf/icpc18short.pdf][grano2018empirical]])
  + TODO

**** Generating natural language descriptions for software artifacts
***** Surveys
- /Survey of Methods to Generate Natural Language from Source Code/ 🌟
  ([[http://www.languageandcode.org/nlse2015/neubig15nlse-survey.pdf][neubig2016survey]])
  1. Survey papers
    - recommends [[nazar2016summarizing]]
  2. Generation Methods
    1. manual rules/templates
      + SWUM [[hill2009automatically]]&[[sridhara2010towards]]
        - test cases [[zhang2011automated]] & [[kamimura2013towards]]
        - changes [[buse2010automatically]] & [[cortes2014automatically]]
        - exceptions [[buse2008automatic]]
      - multiple lines description [[sridhara2011automatically]]
        + not useful, too high level
      - using execution path information [[buse2008automatic]] & [[zhang2011automated]]
        + not useful(?)
  3. +Content Selection Methods+
  4. +Targeted Software Units+
  5. +Training Data Creation+
  6. Evaluation
    - TODO later
- /Summarizing Software Artifacts: A Literature Review/ 🌟
  ([[https://link.springer.com/content/pdf/10.1007%2Fs11390-016-1671-1.pdf][nazar2016summarizing]]) <<nazar2016summarizing>>
  + very complete
- /Automatic Summarising: The State of the Art/
  ([[https://www.sciencedirect.com/science/article/pii/S0306457307000878][jones2007automatic]])

***** Tools for tests
- /Automatically Documenting Software Artifacts/ 🌟
  ([[http://www.cs.wm.edu/~denys/pubs/dissertations/Boyang-thesis.pdf][li2018automatically]])
  + PhD thesis
  + Chapter 4 (p. 109) on tag for unit tests
  + catalog of 21 stereotypes for methods in unit tests
    - 14 JUnit API-Based Stereotypes for Methods in Unit Test Cases
      + Boolean verifier
      + Null verifier
      + Equality verifier
      + Identity verifier
      + Utility verifier
      + Exception verifier
      + Condition Matcher
      + Assumption setter
      + Test initializer
      + Test cleaner
      + Logger
      + Ignored method
      + Hybrid verifier
      + Unclassified
    - 7 C/D-Flow Based Stereotypes for Methods in Unit Test Cases
      + Branch verifier
      + Iterative verifier
      + Public field verifier
      + API utility verifier
      + Internal call verifier
      + Execution tester
      + Empty tester
- /Automatically Documenting Unit Test Cases/ 🌟🌟
  ([[http://www.cs.wm.edu/~denys/pubs/_ICST'16-JUnitTestScribe-CRC.pdf][li2016automatically]]) ([[https://github.com/boyangwm/UnitTestScribe][git]])
  + Survey with developers and projects mining study to justify automatic
    documentation of unit tests
  + uses a SWUM implementation in C#
  + example of templates and placeholders
  + as with other similar works it may not be useful for us
- /Towards Generating Human-Oriented Summaries of Unit Test Cases/ 🌟
  ([[http://www.cs.ubc.ca/~murphy/papers/summarization/icpc13era-t9-p-16545-preprint.pdf][kamimura2013towards]]) <<kamimura2013towards>>
- /Automated Documentation Inference to Explain Failed Tests/
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.700.252&rep=rep1&type=pdf][zhang2011automated]]) <<zhang2011automated>>
  + could be used to improve the documentation and precision of ~try/catch~
    amplification
- /Automatically Identifying Focal Methods under Test in Unit Test Cases/
  ([[https://www.researchgate.net/profile/Mohammad_Ghafari3/publication/295918716_Automatically_Identifying_Focal_Methods_Under_Test_in_Unit_Test_Cases/links/57cd3d5f08ae89cd1e87bf9f.pdf][ghafari2015automatically]])
  + not useful, we are focusing on explaining edge cases

***** Commits/Code changes
- /On Automatically Generating Commit Messages via Summarization of Source Code Changes/
  ([[https://www.researchgate.net/profile/Luis_Cortes11/publication/267326224_On_Automatically_Generating_Commit_Messages_via_Summarization_of_Source_Code_Changes/links/5583f12208ae4738295bd3ca.pdf][cortes2014automatically]]) <<cortes2014automatically>>
  /ChangeScribe: A Tool for Automatically Generating Commit Messages/
  ([[http://www.cs.wm.edu/~denys/pubs/ICSE%2715-ChangeScribeTool-CRC.pdf][linares2015changescribe]])
  + Good entry point for the related work
  + Classifies commit with stereotypes
  + Uses templates for sentences, and fills it with commit stereotypes
    ([[dragan2011using]])
  + lacks 'why' information
- /Using Stereotypes to Help Characterize Commits/
  ([[http://www.cs.kent.edu/~jmaletic/papers/ICSM11.pdf][dragan2011using]]) <<dragan2011using>>
  + Only categorize based on added or deleted methods
- /Towards Automatic Generation of Short Summaries of Commits/
  ([[https://arxiv.org/pdf/1703.09603.pdf][jiang2017towards]])
- /Automatically Generating Commit Messages from Diffs using Neural Machine Translation/
  ([[https://arxiv.org/pdf/1708.09492.pdf][jiang2017automatically]])
  + trying to be less verbose and add context
- /On Automatic Summarization of What and Why Information in Source Code Changes/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7551998][shen2016automatic]])
  + Better than /ChangeScribe/[[cortes2014automatically]]
  + Categories of Commits in Terms of Maintenance Task and Corresponding Description
    (based on [[swanson1976dimensions]]) (why information)
    +-----------------------+----------------------------------+
    | Categories of commits |           Description            |
    +-----------------------+----------------------------------+
    | Implementation        | New requirements                 |
    +-----------------------+----------------------------------+
    | Corrective            | Processing failure               |
    |                       | Performance failure              |
    |                       | Implementation failure           |
    +-----------------------+----------------------------------+
    | Adaptive              | Change in data environment       |
    +-----------------------+----------------------------------+
    | Perfective            | Processing inefficiency          |
    |                       | Performance enhancement          |
    |                       | Maintainability                  |
    +-----------------------+----------------------------------+
    | Non functional        | Code clean-up                    |
    |                       | Legal                            |
    |                       | Source control system management |
    +-----------------------+----------------------------------+
  + What information: description (more like diff (ChangeDistiller) dump) of
    changes
  + only keep information for methods that are called many times
  + boilerplates not interesting
- /Automatically Documenting Program Changes/
  ([[http://web.eecs.umich.edu/~weimerw/p/weimer-ase2010-deltadoc-preprint.pdf][buse2010automatically]]) <<buse2010automatically>>
  + precise description
  + nicely written, but not useful for us
- /Towards a taxonomy of software change/ 🌟
  ([[http://ulsites.ul.ie/csis/sites/default/files/csis_towards_a_taxonomy_of_software_change.pdf][buckley2005towards]])
  + purely about what information
  + nice charts or table to display all possible informations

***** General/Others
- /Comment Generation for Source Code: State of the Art, Challenges and Opportunities/
  ([[https://arxiv.org/pdf/1802.02971.pdf]])
  + TODO
  + Information Retrieval ("analyze the natural language clues in the source
    code") -> not relevant
  + Program Structure Information (summary from important statements) -> not
    relevant(?)
  + Software Artifacts Beyond Source Code (using the social interaction
    revolving around development) -> not relevant
  + Fundamental NLP Techniques -> not relevant
  + Not very useful... "current approach only generate descriptive comments"
- /The Emergent Laws of Method and Class Stereotypes in Object Oriented Software/
  ([[https://etd.ohiolink.edu/!etd.send_file?accession=kent1290570321&disposition=inline][dragan2011emergent]])
  + Excerpt from PhD Thesis
  + Source of the Taxonomy of Method Stereotypes 🌟
  + C++
- /The Dimensions of Maintenance/
  ([[http://www.mit.jyu.fi/ope/kurssit/TIES462/Materiaalit/Swanson.pdf][swanson1976dimensions]]) <<swanson1976dimensions>>
  + Foundational paper
- /JStereoCode: Automatically Identifying Method and Class Stereotypes in Java Code/
  ([[https://dl.acm.org/citation.cfm?id=2351747][moreno2012jstereocode]])
  + Extending Dragan's work <<dragan2011using>> for Java
- /Automatic Documentation Inference for Exceptions/ 🌟
  ([[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.8478&rep=rep1&type=pdf][buse2008automatic]]) <<buse2008automatic>>
  + well written
  + could be used to improve the documentation and precision of ~try/catch~
    amplification
  + nice study of percentage of what and why information in open-source
    projects' commit messages
- /Towards Automatically Generating Summary Comments for Java Methods/ 🌟
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/200/towards-automatically-generating-summary-comments-for-methods.pdf?sequence=3][sridhara2010towards]]) <<sridhara2010towards>>
  (+ PhD thesis)
  - well written
  - SWUM, central lines selection, ...
  - again not exactly useful for us
- /Integrating Natural Language and Program Structure Information to Improve Software Search and Exploration/
  ([[https://search.proquest.com/openview/89d289c5561fc953875cf9d6f223a7cc/1?pq-origsite=gscholar&cbl=18750&diss=y][hill2010integrating]])
  + PhD thesis
  + Source of SWUM
  + SWUM implementation as Eclipse plugin
- /Swummary: Self-Documenting Code/
  ([[https://scholarscompass.vcu.edu/capstone/114/][herbert2016swummary]]) ([[https://github.com/herbertkb/Swummary][git]])
  + focal method extraction -> Swum.NET
- /Automatic Source Code Summarization of Context for Java Methods/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7181703][mcburney2016automatic]])
  + looks very complete but again not quite useful
- /Method Execution Reports: Generating Text and Visualization to Describe Program Behavior/ 🌟🌟
  ([[http://bergel.eu/MyPapers/Beck17a-MethodExecutionReports.pdf][beck2017method]])
  + good list of possible information
  + TODO
- /Towards Automatically Generating Descriptive Names for Unit Tests/
  ([[https://ieeexplore.ieee.org/abstract/document/7582797/][zhang2016towards]])
  + TODO

**** Commits/Code survey
- /What’s a Typical Commit? A Characterization of Open Source Software Repositories/
  ([[https://www.researchgate.net/profile/Huzefa_Kagdi/publication/4349695_What%27s_a_Typical_Commit_A_Characterization_of_Open_Source_Software_Repositories/links/00b7d528a6e2589336000000.pdf][alali2008s]])
  - Useful to know what terms to use
  - According to [[cortes2014automatically]] the most used terms are fix, add,
    test, bug, patch and the most used combinations are file-fix, fix-use,
    add-bug, remove-test, and file-update.
- /On the Nature of Commits/
  ([[https://sci-hub.tw/10.1109/ASEW.2008.4686322][hattori2008nature]])
- /What do large commits tell us? A taxonomical study of large commits/
  ([[http://maveric0.uwaterloo.ca/~migod/846/papers/msr08-hindle.pdf][hindle2008large]])
  + extending [[swanson1976dimensions]]
- /Cognitive Processes in Program Comprehension/
  ([[https://ac.els-cdn.com/016412128790032X/1-s2.0-016412128790032X-main.pdf?_tid=aff39f10-109e-11e8-8c6f-00000aacb360&acdnat=1518513618_e744f6cb72ebf42954fbb25e1eb42220][letovsky1987cognitive]])
  + Foundational paper
- /On the Naturalness of Software/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6227135][hindle2012naturalness]])
  + Code is repetitive and predictable

**** Natural Language Generator
- /SimpleNLG: A realisation engine for practical applications/
  ([[http://www.aclweb.org/anthology/W09-0613][gatt2009simplenlg]])
  + TODO

**** Code Evolution
- /Erlang Code Evolution Control/
  ([[https://arxiv.org/pdf/1709.05291.pdf][arXiv:1709.05291]])
  + TODO

**** Test Case Minimisation
- /Efficient Unit Test Case Minimization/
  ([[https://www.semanticscholar.org/paper/Efficient-unit-test-case-minimization-Leitner-Oriol/7ea90839a908f8a0b171d93fad72bcace2cdf0ad][leitner2007efficient]])
  + TODO
- /Yesterday, my program worked. Today, it does not. Why?/
  ([[https://dl.acm.org/citation.cfm?id=318946][zeller1999yesterday]])
  + TODO

**** Not Relevant
***** Knowledge
- /Poster: Construct Bug Knowledge Graph for Bug Resolution/
  ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7965299][wang2017construct]])
- /Towards the Visualization of Usage and Decision Knowledge in Continuous Software Engineering/
  ([[https://wwwbruegge.in.tum.de/lehrstuhl_1/research/paper/johanssen2017visualization.pdf][johanssen2017towards]])
  + Pretty figures
  + Design of a tool to visualize various kinds of knowledge
- /Method Execution Reports: Generating Text and Visualization to Describe Program Behavior/
  ([[http://bergel.eu/MyPapers/Beck17a-MethodExecutionReports.pdf][beck2017method]])

***** Testing Related
- /SCOTCH: Test-to-Code Traceability using Slicing and Conceptual Coupling/
  ([[https://pdfs.semanticscholar.org/d38a/88ee65b56c2e3e3efc33c727d0990678683c.pdf][qusef2011scotch]])
- /ComTest: A Tool to Impart TDD and Unit Testing to Introductory Level Programming/
  ([[http://ims.mii.lt/ims/konferenciju_medziaga/ITiCSE'10/docs/p63.pdf][lappalainen2010comtest]])

***** Others
- /A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes/
  ([[https://arxiv.org/pdf/1704.04856.pdf][loyola2017neural]])
  + Multiple good citation to papers on NL and SE
- /Automatically Capturing Source Code Context of NL-Queries for Software Maintenance and Reuse/
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/199/Hill09.pdf?sequence=4][hill2009automatically]]) <<hill2009automatically>>
- /How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms/
  ([[https://dl.acm.org/citation.cfm?id=2486788.2486857][panichella2013effectively]])
  + Enhancement that doesn't really interest us
  + "in the context of three different SE tasks: (1) traceability link recovery,
    (2) feature location, and (3) software artifact labeling."
- /Software traceability with topic modeling/
  ([[https://dl.acm.org/citation.cfm?doid=1806799.1806817][asuncion2010software]])
  + "navigate the software architecture and view semantic topics associated with
    relevant artifacts and architectural components"
- /Automatically Detecting and Describing High Level Actions within Methods/
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/204/automatically-detetct-and-describe-high-level-actions-in-methods.pdf?sequence=1][sridhara2011automatically]]) <<sridhara2011automatically>>
  + too high level
- /Automatic Generation of Natural Language Summaries for Java Classes/
  ([[http://servo.cs.wlu.edu/pubs/bitstream/handle/id/285/icpc13summaries-submitted.pdf?sequence=1][moreno2013automatic]])
- /Using Method Stereotype Distribution as a Signature Descriptor for Software Systems/
  ([[http://www.cs.kent.edu/~ndragan/ICSM09.pdf][dragan2009using]])
- /Reverse Engineering Method Stereotypes/
  ([[http://www.cs.kent.edu/~jmaletic/papers/ICSM06.pdf][dragan2006reverse]])
- /Supporting Program Comprehension with Source Code Summarization/
  ([[https://www.researchgate.net/profile/Jairo_Aponte/publication/215739380_Supporting_program_comprehension_with_source_code_summarization/links/554771110cf2e2031b36b7fd.pdf][haiduc2010supporting]])
  - motivations
- /Natural Language-based Software Analyses and Tools for Software Maintenance/
  ([[https://users.drew.edu/ehill1/papers/lncs12.pdf][pollock2009natural]])
  + more about analysis than generation

** Contribution
*** Minimisation
*** Focus
*** Replace original test or keep both
*** Explanation
**** Slicing
**** Natural Description
*** Ranking


* Development
[[https://github.com/STAMP-project/dspot/issues/187][Issue]]
[[https://github.com/STAMP-project/dspot/issues/362][Issue]]


* Global Goals [2/2]
** DONE Report <2018-06-08 Fri 12:00>
- [X] Thanks all the team in report (Benjamin, Benoit, Martin)
** DONE Defense <2018-06-27 Wed 14:00>
*** DONE Talk @ Workshop Software Engineering Research <2018-03-08 Thu 10:00>--<2018-03-08 Thu 10:20>
- Room 4523
- 10 minutes talk
- [[https://docs.google.com/document/d/1NL3FGr_ruYRTY4824mHitkjwitKyBVcaddZLJpbtztA/edit]]
- /Mandatory/ slides [4/4]
  + [X] Problem statement
  + [X] Experiment protocol
  + [X] Experiment results
    - no results yet 😞
  + [X] Related works
*** DONE Talk @ Workshop Software Engineering Research <2018-05-08 Tue>
*** Defense Rehearsal @ ENS <2018-06-22 Fri>
- Actually, no


* Journal [21/21]
** DONE Preliminary Bibliographical Work <2017-09-18 Mon>--<2018-02-07 Wed>
*** Things Done
- Meeting with Benoit <2017-09-22 Fri>
  + [[https://github.com/STAMP-project/dspot/issues/187][1]], [[https://github.com/STAMP-project/dspot/issues/129][2]], [[https://github.com/STAMP-project/dspot/issues/54][3]] issues for possible work to do
  + 1 possible work: explain if a mutant isn't killed because of oracle or input
  + focus on mutation (e.g. mutation score)
  + work will be on [[https://github.com/STAMP-project/dspot][Dspot]] and [[https://github.com/STAMP-project/pitest-descartes][PIT]].
- Read [[http://massol.myxwiki.org/xwiki/bin/view/Blog/MutationTestingDescartes][blog on PIT and Descartes]]
  + Sum up PIT/Descartes
  + List of wanted features
- Meeting with Benoit <2017-11-23 Thu>
  + The purpose of DSpot has shifted right?
    - interesting to talk about the history in bibliography? No, there is a new
      paper
  + Enough space to talk about related work? present a few papers in details and
    cite others
  + Current organisation of bibliography
    - General techniques
      + Definitions
      + Mutants
      + etc
    - Useful tools
      + DSpot
  + do extensive evaluation (comparison from scratch vs amplification)
  + find literals to help tests
  + add mutation operator for specific data structures
  + stack mutations
  + add explanations
  + 3 big open problems
- Meeting with Benoit <2017-12-22 Fri>
  + reduce only the generated tests
  + big question: minimal generated tests
    - pre or post treatement
    - order of presenting PRs
    - this is the big question
    - we don't want to touch the original suite
    - we want the programmer to understand the new tests
  + add an example of junit test
  + talk about the trend of genetic improvement
  + don't necesseraly cite /Automatic software diversity in the light of test
    suites/ and /Tailored source code transformations to synthesize
    computationally diverse program variants/
- Talk rehearsal <2018-01-28 Mon 08:30>, notes by Vladislav
  - More illustrations (workflow graph?)
  + Check the test case example (too complicated for not much, not really java)
  + Year and conference acronym in footcite
  + Careful with lambdas for TDR (check with supervisor)
  + More details on commits/pull requests and emphasize the importance of
    developers reviewing generated tests
  + Slide 10 -> ugly (different spacings)
  + Stacking operators: explanation too sparse
  + 4th point in conclusion slide too vague. Not just the goal but also the mean
    to achieve it
- [[https://blog.acolyer.org/2018/01/23/why-is-random-testing-effective-for-partition-tolerance-bugs/]]

*** Blocking Points

*** Planned Work [6/6]
- [X] Read papers
- [X] Meeting with Benoit <2017-09-22 Fri 15:00-15:30>
- [X] Meeting with Benoit <2017-11-23 Thu 15:00-16:00>
- [X] Send link to repo
- [X] Ask Maud about plane tickets refund
- [X] Meeting with Benoit <2017-12-22 Fri 10:30-11:30>


** DONE Week 1 & 2 <2018-02-07 Wed>--<2018-02-18 Sun>
*** Things Done
- Wrote the little example of use of Spoon (I simply added it in [[https://github.com/SpoonLabs/spoon-examples][spoon-examples]])
#+NAME: RemoveIf
#+BEGIN_SRC java
package fr.inria.gforge.spoon.transformation;

import spoon.processing.AbstractProcessor;
import spoon.reflect.code.*;

/**
 * Removes if when there is no else and if the body consists only of a return
 *
 * @author Simon Bihel
 */
public class RemoveIfReturn extends AbstractProcessor<CtIf> {

    @Override
    public void process(CtIf element) {
        CtStatement elseStmt = element.getElseStatement();
        if (elseStmt == null) { return; } // should not be an else

        CtStatement thenStmt = element.getThenStatement();
        if (thenStmt instanceof CtReturn) { // simple case with directly a then statement
            element.replace(thenStmt);
            return;
        }
        if (thenStmt instanceof CtBlock) { // case with a block which first statement is a return
            CtStatement firstStmt = ((CtBlock) thenStmt).getStatement(0);
            if (firstStmt instanceof CtReturn) {
                element.replace(thenStmt);
            }
        }
    }
}
#+END_SRC
#+Name: RemoveIfTest
#+BEGIN_SRC java
#+END_SRC
- [[https://clang-analyzer.llvm.org/][Clang static analyzer]] for windows
  + Clang is painful to install on Windows... It requires llvm and Microsoft
    Visual Studio. And there is no other choice than building from source. And
    it requires Perl to run.
  + Should probably use [[http://cppcheck.sourceforge.net/][CPPcheck]]
  + Cppcheck has a GUI and an installer for Windows. 👍
  + example of bugs [[http://courses.cs.vt.edu/~cs1206/Fall00/bugs_CAS.html]]
  + no bug in the provided code
- Software Maintenance seems to be an important keyword/field for the
  documentation of code
- To what extent are documenting source code changes useful for us?
  + Only few changes made by DSpot
  + The source of the change is a tool, not a human
  + Still useful to see how they formulate features in natural language
  + DSpot doesn't add new features, we want the purpose of enhanced tests.
  + Don't really care about Pyramid method because it compares with human
    written messages
- GitHub's [[https://help.github.com/articles/creating-a-pull-request-template-for-your-repository/][PR templates]] are just plain text templates.
- Went through papers that cited ChangeScribe. Went partly through citations by
  ChangeScribe.
- Spent a lot of time on generating natural language from source code
- Submitted a [[https://github.com/jceb/vim-orgmode/pull/291][fix]] for a bug in vim-orgmode
- Natural Language Generators
  + found on github, for java
    1. [[https://github.com/simplenlg/simplenlg][SimpleNLG]]
      - 410 stars, 215 citations
      - Seems to be just what we need
    2. [[https://github.com/kariminf/nalangen][NaLanGen]]
      - 2 stars
  + ChangeScribe seems to use a homemade generator
- "The Software Word Usage Model (SWUM) is one of the first models of this type,
  and can be used for converting Java method calls into natural language
  statements (Hill et al., 2009)."
- Looking at the code of DSpot to get info on generated tests
  + looks like a list of amplified test are generated and you don't know what
    was the amplifier

*** Blocking Points
- Is it useful to explore approaches for augmenting the context provided by
  differencing tools?

*** Planned Work [6/12]
- [X] Read papers
- [ ] should I register for ICST? and +ICSE+? -> Yes, talk/remind Benoit
  - Not eligible for [[http://www.es.mdh.se/icst2018/kaist-diversity-student-travel-awards/]]
- [X] Sign papers grant
- [X] Is there a Slack or something?
- [-] Get familiar with Spoon
  + [ ] Read paper
  + [-] Little project, remove ~if~ when there is no ~else~ and the body is
    just a ~return~.
    - [X] Write the program
    - [ ] Write tests
- [ ] Get familiar with Dspot
  + [ ] Running it
  + [ ] Contributing
    - [ ] Pick issues
    - [ ] Fix them
- [-] See /boiler-plates/ for NLP way of building sentences.
  + a.k.a templates, placeholder templates
  + [ ] Search for papers and read them
  + [X] Search for tools
- [X] Sign contract with KTH Relocation <2018-02-13 Tue 14:00>--<2018-02-13 Tue 15:30>
- [X] Categorize papers of preliminaries
- [X] Lookup what static analysis is possible with +clang+ Cppcheck [100%]
  + [X] find tools
  + it is for mechatronics students who write small programs for arduinos
  + show them what tests are and what's possible to discover bugs
  + [X] Think of what they could be taught
  + [X] Test Cppcheck on a windows machine
    - [X] Install windows on the small computer
    - [X] Test the code provided in the course
- [ ] Go to Entré for badge and PIN code outside working hours
- [ ] Run tools that I encounter in papers


** DONE Week 3  <2018-02-19 Mon>--<2018-02-25 Sun>
*** Things Done
- Work on DSpot documentation
- Read reviews of bibliographic report
- How to remember what amplification has been applied?
  + +Go through logs+
    - nothing useful in them
  + Comments directly in the code
    - name of the amplifier used in the line before
    - could easily be enriched if necessary
  + +Enrich test methods with a new parameter+
    - last resort
- A =json= file summarizes killed mutants (with their location)
- Need to keep focus
#+BEGIN_QUOTE
To select the new test case to be proposed as pull request, we look for an
amplified test that kills mutants which are all located in the same method.
#+END_QUOTE
(this was done manually)
- Need for automated minimization
#+BEGIN_QUOTE
A second point in the preparation of the pull request relates to the length of
the amplified test. Once a test method has been selected as a candidate pull
request, we analyze it and manually make it clearer and more concise, we call
this process the manual minimization of the amplified test. We note that
automated min- imization of amplified tests is an interesting area of future
work, left out of the scope of this paper.
#+END_QUOTE
- SWUM is really about analysis. Trying to reformulate things without making
  sense of them.
- Possible title: Adaptation of Amplified Unit Tests for Human Comprehension
- [[https://github.com/abb-iss/Swum.NET][Swum.NET]]
#+BEGIN_QUOTE
UnitTestScribe also uses SWUM.NET to generate a general NL description for each
unit test case method. SWUM.NET captures both linguistic and structural
information about a program, and then generates a sentence describing the
purpose of a source code method.
#+END_QUOTE
- Started writing
- Made a [[https://github.com/rhysd/vim-grammarous/pull/59][PR]] for vim-grammarous
- [[https://github.com/STAMP-project/dspot/issues/54][Discussion]] on how to minimize generated tests

*** Blocking Points
- [X] Where is the "keep test that kills mutants all located in the same
  method"? Seems to be implemented reading the paper, but [[https://github.com/STAMP-project/dspot/issues/130][issue]] still open and
  it proposes a solution that seems different than just looking at the json file
  at then end of the process.
  + it was done manually

*** Planned Work [7/12]
- [X] Read papers
- [ ] Register for ICST
- [-] +Get familiar with+ Dspot [1/6]
  + [X] Running it
  + [ ] Contributing
    - [ ] Pick issues
    - [ ] Fix them
  + [-] Write documentation [2/4]
    - [-] Key methods [3/5]
      + [X] Assertion generation [2/2]
        - [X] ~AssertGenerator~
        - [X] ~MethodsAssertGenerator~
      + [-] Input amplification [1/2]
        - [X] glue
        - [ ] amplifiers
      + [X] Pre-amplification
      + [X] Amplification
      + [-] Compilation & run [2/3]
        - [X] ~compileAndRunTests~
        - [X] ~buildClassForSelection~
        - [ ] ~TestCompiler~
    - [X] Rename ~amplifyTests~ to express the fact that it is only doing input
      amplification
    - [ ] ~compileAndRunTests~
      + [ ] Why return ~null~ when not all methods were compilable or some
        tests failed?
    - [X] Renaming plural variables
  + [ ] Work on removing all deprecated classes in stamp [0/1]
    - [ ] Remove unused deprecated methods of ~TestSelector~
  + [ ] More precise ~try/catch~?
    - Would that be useful? Feasible?
  + [ ] Extract hard-coded amplifications messages
- [X] Lab access denied outside working hours
  + [X] Go to Entré
  + [X] Go again to Entré
  + [X] Send email to request access to the lab
    - resend
  + [X] Resolved
- [X] Run tools that I encounter in papers
  + tools not really useful are they(?)
  + closing this for now
- [X] Find a way to know which amplifications have been applied and/or how to
  implement it
- [X] Make DHELL [[https://github.com/STAMP-project/dhell/pull/3][PR]] maven compiler version
  + [[https://github.com/spring-guides/gs-maven/issues/21]]
- [-] Start writing [0/4]
  + [-] Problem statement
    - [X] scientific
      + quite short
    - [ ] technical
  + [-] Comparison with works on description
    - [X] Explaining what they do
      + badly written
      + quite short
    - [ ] Why we can't apply them for our work
  + [ ] Comparison with works on test cases minimization
    - [ ] Explaining what they do
    - [ ] Why we can't apply them for our work
  + [ ] Whether using an NLG is useful
- [X] +Start doing a simple NL commit messages generator+
  + for later, first we need minimization
- [X] Maybe reorganize the references on descriptions
- [ ] Read about identify essential parts of a test for killing a specific
  mutant
- [ ] Search for papers on mutation testing and same location targeting


** DONE Week 4  <2018-02-26 Mon>--<2018-03-04 Sun>
*** Things Done
- Added git hook to commit the html version of the reporting
- Explored the use of slicing to detect the cause of new killed mutant
  + Need observation-based slicing with mutation score(?)
- Nothing on summarization and mutation testing
  + You usually think the other way around, what do I need to do in order to
    kill this new mutant
- [[https://github.com/srcML/srcSlice][srcSlice]] not supporting Java ([[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7883355&tag=1][paper]])
- [[https://github.com/hammacher/javaslicer][JavaSlice]] does not support Java 8
- [[http://indus.projects.cs.ksu.edu/projects/kaveri.shtml][Kaveri]] (Indus Java Program Slicer) old and eclipse plugin
- [[http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8080067<Paste>][JavaBST]] not available ? paper badly written
- [[http://wala.sourceforge.net/wiki/index.php/Main_Page][WALA]]
- Fixed org export and also pull on server
- Starred every vim plugin I use with Github's API and [[https://github.com/PyGithub/PyGithub][PyGitbub]]
- Explored end-user description of Pitest mutators
  + Pitest has user-friendly mutators, now the question is how to use/access
    them
-
#+BEGIN_SRC sh
cd .. && mvn clean package -DskipTests && cd dhell && mvn clean package && java -jar ../dspot/target/dspot-1.0.6-SNAPSHOT-jar-with-dependencies.jar -p ./dspot.properties -i 1 -t eu.stamp.examples.dhell.HelloAppTest -a MethodAdd --verbose && vim dspot-out/eu.stamp.examples.dhell.HelloAppTest_mutants_killed.json
#+END_SRC

*** Blocking Points
- NL commit message generator
  + how to know which amplifications were applied?
- What is a program/test slice for a mutation score criterion?
  + dataflow slice starting from the killing assertion

*** Planned Work [3/9]
- [ ] Register for ICST
- [-] Dspot [2/5]
  + [X] Contributing
  + [X] Write documentation [2/2]
    - [X] Key methods [2/2]
      + [X] Input amplification
        - [X] amplifiers
      + [X] Compilation & run
        - [X] ~TestCompiler~
          + no need
    - [X] ~compileAndRunTests~
      + [X] Why return ~null~ when not all methods were compilable or some
        tests failed?
        - Created an [[https://github.com/STAMP-project/dspot/issues/336][issue]]
    - [[https://github.com/STAMP-project/dspot/pull/337][PR]]
  + [ ] Work on removing all deprecated classes in stamp
    - [ ] Remove unused deprecated methods of ~TestSelector~
  + [ ] More precise ~try/catch~?
    - Would that be useful? Feasible?
  + [ ] Extract hard-coded amplifications messages
- [ ] Start writing [0/4]
  + [ ] Problem statement
    - [ ] technical
  + [ ] Comparison with works on description
    - [ ] Why we can't apply them for our work
  + [ ] Comparison with works on test cases minimization
    - [ ] Explaining what they do
    - [ ] Why we can't apply them for our work
  + [ ] Whether using an NLG is useful
- [X] Read about identify essential parts of a test for killing a specific
  mutant
- [X] Search for papers on mutation testing and same location targeting
- [-] Start doing a simple NL commit messages generator [0/2]
  - [ ] DSpot automated PR
  - [-] Simple PR description [3/4]
    + [X] Add a field in the killed mutants ~json~ file
    + [X] Print it
      - done automatically
    + [X] Stupid message
    + [ ] Long stupid description
      - [ ] Get what amplifications were applied
      - [ ] done
- [X] Replace ~fr.inria.stamp~ with ~eu.stamp~
  + [[https://github.com/STAMP-project/dspot/pull/339][PR]]
- [ ] Classification of mutators
- [-] Integrate WALA to compute a slice per new mutant [1/4]
  + [X] Need a more precise location for the mutant location
    - +column number+
      + not available
    - maybe I don't need it
  + [ ] Need to know the killing assertion
    - [ ] Add a trace of this when a test is kept
  + [ ] Adding as dependency
  + [ ] Use it


** DONE Week 5  <2018-03-05 Mon>--<2018-03-11 Sun>
*** Things Done
- Tried to use Sourcetrail
  + Needed to run ~mvn install -DskipTests -Djacoco.skip=true~
  + displayed no references or class
- Worked on presentation for the workshop
- Proposed mutators taxonomy
  + Literal change
  + Object change
  + New assertion
- Meeting with Benoit
  + in commit message, talk about bugs instead of mutants
  + 3 steps
    - oracle enhancement only
    - new input
    - combination
  + write why the problem is difficult
  + write different kinds of message with each a specific focus
  + maybe compare trace of amplified test vs original
  + study commit messages related to tests
- ~git log --grep "^Tested"~

*** Blocking Points
- [X] What will the *scientific* contribution be?
  + Software Engineering is often at the border.
  + We tackle complex problem, that the industry is not particularly interested
    in, at least directly.
  + applying existing methods and see if they scale or just that they can be
    implemented, is a contribution in itself
- [X] What kind of evaluation?
  + survey
  + performance
  + comparison with study of repos

*** Planned Work [3/10]
- [X] Talk @ Workshop Software Engineering Research <2018-03-08 Thu 10:00>--<2018-03-08 Thu 10:20>
  + Workshop <2018-03-08 Thu 09:30>--<2018-03-08 Thu 12:30>
  + Room 4523
- [ ] Register for ICST
- [ ] Dspot [0/1]
  + [ ] Extract hard-coded amplifications messages
- [X] Start writing [4/4]
  + [X] Problem statement
    - [X] technical
  + [X] Comparison with works on description
    - [X] Why we can't apply them for our work
  + [X] Comparison with works on test cases minimization
    - [X] +Explaining what they do+
      + rephrase a description from a survey or something
    - [X] Why we can't apply them for our work
  + [X] Whether using an NLG is useful
- [ ] Start doing a simple NL commit messages generator
  + [ ] Long stupid description
    - [ ] Get what amplifications were applied
    - [ ] done
- [ ] Classification of mutators
- [-] Integrate WALA to compute a slice per new mutant [1/4]
  + [X] Need a more precise location for the mutant location
    - +column number+
      + not available
    - maybe I don't need it
  + [ ] Need to know the killing assertion
    - [ ] Add a trace of this when a test is kept
  + [ ] Adding as dependency
  + [ ] Use it
- [X] Fix [[https://github.com/STAMP-project/dspot/issues/336]]
  + [[https://github.com/STAMP-project/dspot/pull/350][PR]]
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing


** DONE Week 6  <2018-03-12 Mon>--<2018-03-18 Sun>
*** Things Done
- Mutation score is deterministic
  + [[https://groups.google.com/forum/#!topic/pitusers/McG9vipiB-A]]
  + couple of caveat
    - timeouts
    - test order dependencies
    - non deterministic code
- Code maintenance
- Identifying amplifications

*** Blocking Points
- [X] How to detect an amplification that modifies a statement?
  + added amplification -> easy
  + modifying amplification -> ?
    - maybe they have tags/annotations?
      + maybe I could implement that
  + use annotations during amplification process to "tag" amplified statements
- [X] What about a change listener to detect amplifications? and easier
  amplification counter
  + it is silly because /we/ are applying amplifications
  + and big overhead
  + No, use annotations

*** Planned Work [8/17]
- [X] Change apartment <2018-03-15 Thu>
  + [X] move out, hotel -> university
  + [X] retrieve keys <2018-03-15 Thu 12:00>--<2018-03-15 Thu 16:30>
  + [X] move in
- [ ] Register for ICST
  + [[http://www.es.mdh.se/icst2018/registration/]]
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [1/2]
  + [[https://github.com/STAMP-project/dspot/issues/362#issuecomment-372384168]]
    - it is possible to explore the AST and get the amplifications
  + [X] documentation [4/4]
    - [X] ~reduce~
    - [X] ~ampTestToParent~
    - [X] ~tmpAmpTestToParent~ [5/5]
      + [X] is it a buffer to add new relations after applying mutators?
        - yes, to have rounds
      + [X] why isn't it used everywhere?
        - ~ampTestToParent~ is directly modified
      + [X] returns the input without modification, need refactor
      + [X] +when it is used somewhere, amplification counter not incremented+
        - I was wrong
      + [X] document it
    - [X] ~StatementAdd~ doesn't increment amplification counter
    - these issues were resolved
  + [ ] implement annotations
  + [ ] explore trees to find amplifications
- [X] [[https://github.com/STAMP-project/dspot/issues/364][Clean amplification code]] [8/8]
  + [X] amplification counter increment during cloning
    - [X] two methods
      - [X] refactor i-amp
      - [X] refactor a-amp
    - [X] remove original public method and make sure everything work
    - [X] clean imports
  + [X] rework assertion counter because 1 clone can mean many a-amplifications
    - don't increment counter in during cloning
  + [X] removing an assertion means =+1= for the counter?
    - no
  + [X] parenting link
    - [X] update parenting link during cloning
    - [X] remove updating outside
    - [X] remove plain getter
    - [X] load buffer before starting
  + [X] parenting map loading is ugly
    - yeah, well...
  + [X] documentation
  + [X] tests for verifying counter?
    - with report
  + [X] close issue in message
- [X] +rename ~Amplifier~ to ~InputAmplifier~+
  + too much conflicts
- [X] +add up amplifications of parents?+
  + no if a parent has amplifications it is reported
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [X] Ask for access outside working hours
- [X] Respond to [[https://github.com/STAMP-project/dspot/issues/54]]
- [X] Understand [[https://github.com/STAMP-project/dspot/pull/360]]
- [ ] Work on report
  + [ ] use explicit definitions
  + [ ] work on background sections
  + [ ] add Java examples
  + [ ] insist on the distinction between what and why information
  + [ ] describe thoroughly the oracle problem
- [X] Read papers
- Meeting with Benoit <2018-03-16 Fri>
  + Make a formal proposal of natural language description
  + Ask people what they think about it
  + Ask (Simon U, Spoon) (, XWiki) (, SAT4J) what they think of my proposal
  + Difficulties to evaluate because there isn't a lot of material (DSpot isn't
    an established tool).
- [ ] formal description of the NL description
- [ ] ask for opinions


** DONE Week 7  <2018-03-19 Mon>--<2018-03-25 Sun>
*** Things Done
- [[https://stackoverflow.com/questions/14040/developer-testing-vs-qa-team-testing-what-is-the-right-division-of-work]]
- Precise description of NL amplification description
  1. High level description:
    - Enhancement or rework
      1.
        #+BEGIN_QUOTE
        [TEST] Enhancement of <original test's name>.
        #+END_QUOTE
      2.
        #+BEGIN_QUOTE
        [TEST] New test.
        #+END_QUOTE
    - Part of the system where mutants are
      #+BEGIN_QUOTE
      Target <method with mutants>.
      #+END_QUOTE
  2. Slice for each new mutant killed, starting from the killing assertion
    1. New oracle
      + NL paraphrase of the assertion
        #+BEGIN_QUOTE
        The <checked property> is checked.
        #+END_QUOTE
      + NL description of the /impact/ of the mutant
        - variables with different values
          #+BEGIN_QUOTE
          If <changed variables> have different values,
          #+END_QUOTE
        - branches differences
          #+BEGIN_QUOTE
          and <different branches> are explored, the test can detect them.
          #+END_QUOTE
    2. New behaviour
      1. Enhancement (Old mutants are still detected)
        + new interactions with objects
          #+BEGIN_QUOTE
          New interactions with <object> using <methods>.
          #+END_QUOTE
        + if also new oracle enhancement
          - new branches during execution
            #+BEGIN_QUOTE

            #+END_QUOTE
      2. Rework
        + unit test documentation
          #+BEGIN_QUOTE
          <methods> are called on <object>.
          #+END_QUOTE
- Learned about the visitor pattern. That name is regrettably confusing.
- Mutant description
  + Whole method removal
    - "this method was previously not tested in a scenario where it is useful"
  + change of condition
    - "these branches were previously not tested in a scenario where they are
      useful"
  + change to value of variable
    - "this variable was previously not tested in a scenario where it is useful"
  + DON'T DESCRIBE MUTANTS
    - too complicated, huge repercussions, little insights
    - starting to question the relevance of focus on mutants in the same method
      + should see it the other way, check properties for this method
- Got a cold 🤧

*** Blocking Points
- [ ] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
- [ ] Assertion log, can there be REMOVE+ADD instead of MODIFY?

*** Planned Work [0/12]
- [ ] Register for ICST
  + [[http://www.es.mdh.se/icst2018/registration/]]
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [6/9]
  + [X] +implement annotations+
  + [X] +explore trees to report amplifications+
  + [X] call counter in `inputAmplification`
    - [[https://github.com/STAMP-project/dspot/pull/374]]
  + [X] +call counter in assert generator calling method+
    - it is already simple enough in generator
  + [X] use another counter to keep the pointer to AST nodes of amplifications
    - use [[http://spoon.gforge.inria.fr/mvnsites/spoon-core/apidocs/spoon/experimental/modelobs/ActionBasedChangeListenerImpl.html]]
  + [ ] add categories of amplifiers
    - ADD
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
    - REMOVE
  + [X] +Collect them when calling counter+
  + [ ] write test to make sure every amplifier includes the counter update
  + [ ] write test to make sure every amplifier logs the amplifications
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [ ] Work on report
  + [ ] use explicit definitions
  + [ ] work on background sections
  + [ ] add Java examples
  + [ ] insist on the distinction between what and why information
  + [ ] describe thoroughly the oracle problem
- [ ] Read papers
- [ ] formal description of the NL description
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)


** DONE Week 8  <2018-03-26 Mon>--<2018-04-01 Sun>
*** Things Done
- I did so much crap in the PR to refactor parent map 😣
- Discovered the field of cognitive support for unit test comprehension
  + why didn't I hear about that before???
- ID of mutator [[https://github.com/STAMP-project/dspot/commit/c0f7231db083144efd1921521eedffa466a2b167][now]] available in report
- Worked on the report

*** Blocking Points
- [ ] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
- [ ] Assertion log, can there be REMOVE+ADD instead of MODIFY?
- [ ] Should I focus solely on mutants description, amplifications descriptions
  or test case as a whole?
- [ ] Need to identify the main object that is interacted with

*** Planned Work [2/12]
- [X] Register for ICST
  + [[http://www.es.mdh.se/icst2018/registration/]]
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [1/4]
  + [X] use another counter to keep the pointer to AST nodes of amplifications
    - use [[http://spoon.gforge.inria.fr/mvnsites/spoon-core/apidocs/spoon/experimental/modelobs/ActionBasedChangeListenerImpl.html]]
  + [ ] add categories of amplifiers
    - ADD
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
    - REMOVE
  + [ ] write test to make sure every amplifier includes the counter update
  + [ ] write test to make sure every amplifier logs the amplifications
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [ ] Work on report
  + [ ] use explicit definitions
  + [ ] work on background sections
  + [ ] add Java examples
  + [ ] insist on the distinction between what and why information
  + [ ] describe thoroughly the oracle problem
- [X] Read papers
- [ ] formal description of the NL description
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)


** DONE Week 9  <2018-04-02 Mon>--<2018-04-08 Sun>
*** Things Done
- Spent time on UCL PhD application and IELTS

*** Blocking Points
- [ ] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
- [ ] Assertion log, can there be REMOVE+ADD instead of MODIFY?
- [ ] Should I focus solely on mutants description, amplifications descriptions
  or test case as a whole?
- [ ] Need to identify the main object that is interacted with

*** Planned Work [5/17]
- [X] Plan ICST trip
  + [X] +hotel+
    - going to take the train everyday (the train alone is 1h, one way)
  + [X] train
- [ ] Get reimbursed for ICST
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [1/4]
  + [X] use another counter to keep the pointer to AST nodes of amplifications
    - use [[http://spoon.gforge.inria.fr/mvnsites/spoon-core/apidocs/spoon/experimental/modelobs/ActionBasedChangeListenerImpl.html]]
  + [ ] add categories of amplifiers
    - ADD
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
    - REMOVE
  + [ ] write test to make sure every amplifier includes the counter update
  + [ ] write test to make sure every amplifier logs the amplifications
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [-] Work on report [1/6]
  + [ ] use explicit definitions
  + [ ] work on background sections
  + [ ] add Java examples
  + [ ] insist on the distinction between what and why information
  + [ ] describe thoroughly the oracle problem
  + [X] add papers from 'General/Others'
- [X] Read papers
- [ ] formal description of the NL description
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [ ] SCOTCH might actually be useful
  + check other 'Not Relevant' papers
- [X] Meeting with Benoit <2018-04-03 Tue>
  + +Preparation for ICST+
    - hotel or train everyday?
      + don't know
  + +my future+
    - Interesting PhD at UCL but already 3 internships that went badly
      + have to be *motivated*, ask questions
  + +citing blog posts+
    - yes
  + no subjective comment
  + a test is a subset of observable responses that are equal to specification
  + a program is complete but not specifications
  + good introduction to talk about abstract definition of verification
  + need to select subset of R (in def of test activities)
  + encapsulation principle hides stuff which means you can't observe them
    - if something is private you can't test it
  + delegations is difficult to test
  + extremely difficult to define levels of tests
  + how do you know you have a good oracle
  + fig 3, test inputs are only lines 2-5 & 7
  + be clear about what infos we have about the amplified test, how it is
    collected, etc.
  + speak about coverage more than mutants
  + decide between coverage enhancement, slicing or minimization, text or casual
    relationships
  + need a good use case throughout the thesis
- [X] Questions about the PhD position at UCL
- [X] Why is KTH blocking my ens-rennes mails
  + grey listed
  + [X] send email to support
  + [X] warn benoit and madeleine


** DONE Week 10 <2018-04-09 Mon>--<2018-04-15 Sun>
*** Things Done
- ICST
  + Infer, sapienz
  +
  + 1st talk interesting, good intro on flaky tests
  +
  + Repairnator paper
  + Tests as specifications, or something like that, a paper that gives formal
    definitions of tests
  + Talk about over fitting in thesis
  + Write use cases instead of evaluation with developers
  + Really good keynote for testing in the video game industry
  + Nice paper/presentation by shin hong
  + Good talk of José rollas on my subject
    - Check references
  +
  + Check assumptions generation for mutation testing (mutant assumption)
    (question for talk about mutation compression)
- Code Defenders talk by José Rojàs
- Bachelor and Master workshop
  + Zimin's talk
    - what if you predict a line in another function that is closer to another
      prediction in the good function

*** Blocking Points
- [ ] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
- [ ] Assertion log, can there be REMOVE+ADD instead of MODIFY?
- [ ] Should I focus solely on mutants description, amplifications descriptions
  or test case as a whole?
- [ ] Need to identify the main object that is interacted with

*** Planned Work [3/22]
- [X] ICST <2018-04-10 Tue>--<2018-04-12 Thu>
- [ ] Apply to the PhD position
- [X] Check advices from last week meeting with Benoit
- [ ] Get reimbursed for ICST
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [1/4]
  + [X] use another counter to keep the pointer to AST nodes of amplifications
    - use [[http://spoon.gforge.inria.fr/mvnsites/spoon-core/apidocs/spoon/experimental/modelobs/ActionBasedChangeListenerImpl.html]]
  + [ ] add categories of amplifiers
    - ADD
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
    - REMOVE
  + [ ] write test to make sure every amplifier includes the counter update
  + [ ] write test to make sure every amplifier logs the amplifications
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [-] Work on report [1/6]
  + [X] use explicit definitions
  + [ ] work on background sections
  + [ ] add Java examples
  + [ ] insist on the distinction between what and why information
  + [ ] describe thoroughly the oracle problem
  + [ ] over fitting
- [X] Read papers
- [ ] formal description of the NL description
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [ ] SCOTCH might actually be useful
  + check other 'Not Relevant' papers
- [ ] read code defenders papers
- [ ] check Repairnator paper
- [ ] Tests as specifications, or something like that, a paper that gives formal
  definitions of tests
- [ ] Use cases instead of evaluation
- [ ] Check mutant assumption
- [ ] Read José Rojas's ICST paper


** DONE Week 11 <2018-04-16 Mon>--<2018-04-22 Sun>
*** Things Done

*** Blocking Points
- [ ] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
- [ ] Assertion log, can there be REMOVE+ADD instead of MODIFY?
- [ ] Should I focus solely on mutants description, amplifications descriptions
  or test case as a whole?
- [ ] Need to identify the main object that is interacted with

*** Planned Work [8/21]
- [X] Change top mattress <2018-04-16 Mon 13:00>
- [ ] Apply to the PhD position
- [ ] Get reimbursed for ICST
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [1/4]
  + [X] use another counter to keep the pointer to AST nodes of amplifications
    - use [[http://spoon.gforge.inria.fr/mvnsites/spoon-core/apidocs/spoon/experimental/modelobs/ActionBasedChangeListenerImpl.html]]
  + [ ] add categories of amplifiers
    - ADD
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
    - REMOVE
  + [ ] write test to make sure every amplifier includes the counter update
  + [ ] write test to make sure every amplifier logs the amplifications
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [-] Work on report [2/6]
  + [X] work on background sections
  + [ ] add Java examples
  + [X] insist on the distinction between what and why information
  + [ ] describe thoroughly the oracle problem
  + [ ] over fitting
  + [ ] ugly C#
- [X] Read papers
- [ ] formal description of the NL description
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [X] SCOTCH might actually be useful
  + check other 'Not Relevant' papers
- [ ] read code defenders papers
- [X] check Repairnator paper
  + example of software development bot
- [X] Tests as specifications, or something like that, a paper that gives formal
  definitions of tests
- [X] Use cases instead of evaluation
- [X] Check mutant assumption
  + it's mutation applied at the software design level
- [X] Read José Rojas's ICST paper


** DONE Week 12 <2018-04-23 Mon>--<2018-04-29 Sun>
*** Things Done
- Proofread Long's paper
- Mattias' presentation

*** Blocking Points
- [ ] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
- [X] Assertion log, can there be REMOVE+ADD instead of MODIFY?
  + there seems to be a lot of MODIFY
- [ ] Should I focus solely on mutants description, amplifications descriptions
  or test case as a whole?
- [ ] Need to identify the main object that is interacted with
- [ ] Ugly to pass the ~AmplificationListener~ around

*** Planned Work [3/17]
- [X] Apply to the PhD position
- [X] Get reimbursed for ICST
- [ ] Simple NL commit messages generator
- [ ] Classification of mutators
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Study commit messages related to tests
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [1/5]
  + [X] use another counter to keep the pointer to AST nodes of amplifications
    - use [[http://spoon.gforge.inria.fr/mvnsites/spoon-core/apidocs/spoon/experimental/modelobs/ActionBasedChangeListenerImpl.html]]
      + nooooooooooooooo.........
      + if just a literal is changed then you are passed the literal itself, not
        an AST node
      + no it's all good, don't panic, check the context
      + and then later we can use ~getRoleInParent()~
  + [ ] add categories of modifying amplifiers
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
  + [ ] write test to make sure every amplifier includes the counter update
  + [ ] write test to make sure every amplifier logs the amplifications
  + [ ] identify amplification when writing the JSON report for the test case
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [-] Work on report [1/6]
  + [ ] add Java examples
  + [ ] describe thoroughly the oracle problem
  + [ ] over fitting
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [X] ugly C#
  + [ ] make a table for related works?
- [ ] formal description of the NL description
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [ ] check other 'Not Relevant' papers
- [ ] read code defenders papers
- [ ] concurrent map in amplification listener
- [X] +upgrade jacoco version+
  + not my job, and PIT is probably also not supporting java10
- [-] unspecify tests [4/5]
  + use a Set to avoid failing tests due to the ordering of assertions
  + [X] ~testBuildNewAssertWithComment(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testBuildNewAssert(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testNoInstrumentationOnGeneratedObject(fr.inria.diversify.dspot.assertGenerator.AssertGeneratorHelperTest): expected:<..."test_sd6__1");(..)~
  + [X] ~testWithLoop(fr.inria.diversify.dspot.amplifier.StatementAddTest): expected:<...compute(0);(..)~
  + [ ] find others that could fail
  + problem of readability when it fails


** DONE Week 13 <2018-04-30 Mon>--<2018-05-06 Sun>
*** Things Done
- DSpot on javapoet
#+BEGIN_SRC
project=.
targetModule=.
src=src/main/java/
testSrc=src/test/java
javaVersion=8
outputDirectory=dspot-out/
filter=com.squareup.javapoet.*
#+END_SRC
  + there are new amplified tests but only with new amplifications
- DSpot on mustache.java (9.0) compiler
- Listener only collects a-amplifications, trying something else, like a global
  counter
  + and it was very slow
- First version pr_message_gen
#+BEGIN_SRC sh
python3 main.py -amplog ../dspot/javapoet/dspot-out/com.squareup.javapoet.TypeNameTest_amp_log.json -mutants ../dspot/javapoet/dspot-out/com.squareup.javapoet.TypeNameTest_mutants_killed.json
#+END_SRC
#+BEGIN_SRC sh
for filename in ../dspot/javapoet/dspot-out/*_amp_log.json; do; python3 main.py -test ${filename: : -13}; done;
#+END_SRC

*** Blocking Points
- [X] Assertion count
  + Is the process: remove *all* assertions and generate all possible assertions?
  + if so then all assertions are counted as amplifications
  + it's fine
- [X] Should I focus solely on mutants description, amplifications descriptions
  or test case as a whole?
  + do everything and compare
- [ ] Need to identify the main object that is interacted with
- [X] +Ugly to pass the ~AmplificationListener~ around+
  + stand-alone tool

*** Planned Work [12/19]
- [X] Simple NL commit messages generator
- [X] +Classification of mutators+
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [X] +Study commit messages related to tests+
- [ ] More precise ~try/catch~ would actually be useful for slicing
- [-] Retrieve amplifications [3/4]
  + [ ] add categories of modifying amplifiers
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
  + [X] +write test to make sure every amplifier includes the counter update+
  + [X] +write test to make sure every amplifier logs the amplifications+
  + [X] +identify amplification when writing the JSON report for the test case+
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [-] Work on report [1/5]
  + [ ] add Java examples
  + [ ] describe thoroughly the oracle problem
  + [X] over fitting
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [ ] make a table for related works?
- [X] +formal description of the NL description+
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [X] +check other 'Not Relevant' papers+
- [X] read code defenders papers
- [X] concurrent map in amplification listener
- [-] unspecify tests [4/5]
  + use a Set to avoid failing tests due to the ordering of assertions
  + [X] ~testBuildNewAssertWithComment(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testBuildNewAssert(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testNoInstrumentationOnGeneratedObject(fr.inria.diversify.dspot.assertGenerator.AssertGeneratorHelperTest): expected:<..."test_sd6__1");(..)~
  + [X] ~testWithLoop(fr.inria.diversify.dspot.amplifier.StatementAddTest): expected:<...compute(0);(..)~
  + [ ] find others that could fail
  + problem of readability when it fails
  + [[https://github.com/STAMP-project/dspot/pull/413]]
- [X] +Referees reminder (deadline <2018-05-11 Fri>)+
- [X] Answer Benoit's mail on /Towards Automatically Generating Descriptive Names for Unit Tests/
  + they, partly, analyse the text in the test's body
  + Action
  + Expected Outcome
  + Scenario Under Test
  + we want to explain amplifications
- [X] Meeting Benoit <2018-05-02 Wed 17:00>--<2018-05-02 Wed 18:00>
  + to present
    - l'état de l'art: que font les papiers que tu as lus et qu'est-ce qui
      différencie ton approche des autres
    - l'état de ton outil: où est-il par rapport à DSpot, qu'est-ce qu'il peut
      faire
    - tes plans pour une validation expérimentale de cet outil
    - tes plans d'ici la fin du stage
  + print report
  + discuss [[http://www.ieee-scam.org/2018/][SCAM]] (paper deadline <2018-06-15 Tue>)
    - I'll never have a contribution and a report by <2018-06-08 Fri>, so adding
      a paper on top of that...
  + a table for the related works could still be nice, in addition to the text
  +
  + abstract myself from failing tests
  + hyperlink of the line of the mutant
  + have the generator independent from DSpot
    - serialize the list of amplifications
  + multiple version of generators
  + pick a text extension
  + have a discussion on the different versions
  + use projects from the evaluation in the paper, not dhell
- [X] +mutants reported should only be the new ones+
  + can remove, afterwards, redundant mutants from parent
- [X] remove discarded test from the amplification log


** DONE Week 14 <2018-05-07 Mon>--<2018-05-13 Sun>
*** Things Done
- New command line usage ~python3 main.py -p ../dspot/javapoet/dspot.properties -t com.squareup.javapoet.JavaFileTest~
- for link previews to work on github
  + can't be inside a md link
  + can't have =/./=
- Using distance metric for mutants to express how indirect the mutants
  detection is?
  + that's not a clear and confusing metric
- run DSpot on jsoup
  + very long
  + NPE on minimisation
- run DSpot on twilio
  + way too big
- running DSpot on dhell doesn't yield anything
- Pierre Laperdrix talk on browser fingerprinting
- run DSpot on scribejava-core, no i-amplified test
- tried on jodatime but JUnit too old
- socketio test suite not passing
- javacpp test suite not passing
- google-java-format seems to be way too long
- nothing for fb-contrib
- fess is too slow
- run on xwiki-commons-core
  + [[http://massol.myxwiki.org/xwiki/bin/view/Blog/TestGenerationDspot]]
  + didn't work, NPE, PIT problems, ...
- didn't work on jabref
- didn't work on webdrivermanager

*** Blocking Points
- [ ] Need to identify the main object that is interacted with
- [X] What does it mean to have only a-amp, when the original tests are already
  a-amplified?
  + nah, must be a problem, I don't collect all a-amps

*** Planned Work [6/15]
- [X] Renew SL access card <2018-05-08 Tue>
- [X] Workshop <2018-05-08 Tue>
  + Room 4423
- [X] Study for IELTS
- [X] Withdraw UCL application
  + [X] withdraw online application
  + [X] people to apologise to
    - [X] Justyna
    - [X] Shin
    - [X] ENS profs
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Retrieve amplifications [0/1]
  + [ ] add categories of modifying amplifiers
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
- [ ] Retrieve mutants
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
- [ ] Work on report [0/4]
  + [ ] add Java examples
  + [ ] describe thoroughly the oracle problem
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [ ] make a table for related works?
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [-] unspecify tests [4/5]
  + use a Set to avoid failing tests due to the ordering of assertions
  + [X] ~testBuildNewAssertWithComment(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testBuildNewAssert(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testNoInstrumentationOnGeneratedObject(fr.inria.diversify.dspot.assertGenerator.AssertGeneratorHelperTest): expected:<..."test_sd6__1");(..)~
  + [X] ~testWithLoop(fr.inria.diversify.dspot.amplifier.StatementAddTest): expected:<...compute(0);(..)~
  + [ ] find others that could fail
  + problem of readability when it fails
  + [[https://github.com/STAMP-project/dspot/pull/413]]
- [-] improvements [4/10]
  + [X] group assertions
  + [X] Add links in report
    - from [[https://github.com/mazubieta/gitlink-vim]]
  + [ ] "The new test can detect if toBuilder returns XXX instead of the regular
    value. The original test 'toto' could not detect this fault"
    where XXX is the value injected by the mutation
    - isn't really suitable for other kinds of mutants, and even for
      return-related mutants as what might be interesting is that they change
      the state of the SUT, but don't have a direct relation with the test case
  + [ ] also consider when mutation modifies the state and is detected later
  + [ ] Don't name mutators, only explain the transformation instance
    (i.e. mutator category?)
  + [X] Group mutants that are on the same line
  + [X] show the generated test with a =diff=
    - don't put diff for assertions when there are too much (more than 10)
  + [ ] don't talk about 'new detectable bugs' but about 'assess more behavior
    than original' and/or 'reach more paths than original'
  + [ ] For example, "this new test assesses more behavior as the original: it
    can detect 5 changes in the source code that the original test could not
    detect:" and then show the changes (diffs)
  + [ ] handle ~try/catch~
- [X] Read =dspot.properties= and read various info like the path
- [X] Measure the overhead of the amplification logging
  + javapoet
    - DSpot(=master=): 47m22s
    - DSpot(=collect_amp=): 45m52s 45m50s 52m3s
- [ ] Make better names for generated tests
- [ ] Don't add ~try/catch~ when src_java[:exports code]{@Test(expected = IllegalArgumentException.class)}


** DONE Week 15 <2018-05-14 Mon>--<2018-05-20 Sun>
*** Things Done
- Spoon talk
- [[https://askubuntu.com/questions/623577/no-such-file-or-directory-when-trying-to-remove-a-file-but-the-file-exists]]
- DSpot was not generating inputs... 😰🤬😰🤬😰🤬😰🤬
  + =java -jar ~/internship_amplification/dspot/dspot/target/dspot-1.1.1-SNAPSHOT-jar-with-dependencies.jar --path-to-properties dspot.properties --amplifiers AllLiteralAmplifiers:MethodAdd:MethodRemove:StatementAdd:ReplacementAmplifier --clean --verbose=
- run on javapoet
  + added ~jvmArgs=['-Xmx2048m','-Xms1024m','-XX:-UseGCOverheadLimit']~
    - didn't work
  + bumped to =-Xms16G -Xmx32G=
  + removed
    - too messy
- =java.lang.OutOfMemoryError: GC overhead limit exceeded=
  + removing weak keys (and ~concurrencyLevel~ at 4)
    - didn't change anything with =-XX:-UseGCOverheadLimit=
    - was taking forever without =-XX:-UseGCOverheadLimit=, and couldn't kill it
  + raising ~concurrencyLevel~ to 16 (and keeping weak keys)
    - didn't change anything
  + raising ~concurrencyLevel~ to 16 without weak keys
    - didn't change anything
  + same errors without the ~ampTestToParentName~ map
    - had to put it back because of the NPE
  + removed =MethodRemove= and =AllLiteralAmplifier=
    - was taking forever and couldn't kill it
  + with only =StatementAdd=
    - =java.lang.OutOfMemoryError: GC overhead limit exceeded= after an hour,
      instead of 30 minutes
  +
#+BEGIN_SRC sh
java -jar ~/internship_amplification/dspot/dspot/target/dspot-1.1.1-SNAPSHOT-jar-with-dependencies.jar --test com.squareup.javapoet.ParameterSpecTest --cases equalsAndHashCode --path-to-properties dspot.properties --amplifiers StatementAdd --verbose
#+END_SRC
    - no result
  + =xwiki-commons-crypto-cipher=
#+BEGIN_SRC sh
java -jar ~/internship_amplification/dspot/dspot/target/dspot-1.1.1-SNAPSHOT-jar-with-dependencies.jar --path-to-properties dspot.properties --amplifiers MethodAdd --test-criterion PitMutantScoreSelector --clean --verbose
#+END_SRC
    - reproduced the results of Massol 👍
    - 17min
- ~java.lang.OutOfMemoryError: Java heap space~ with main DSpot(master), all
  amplifiers
- =java.lang.OutOfMemoryError: GC overhead limit exceeded= with main
- =java.lang.OutOfMemoryError: Java heap space= with main DSpot(master), only
  =StatementAdd=, after 1h21min
  DSpot(master), without ~MethodRemove~ and ~AllLiteralAmplifier~
- some much time gone in the wind 🤬
- main DSpot(master)
#+BEGIN_SRC sh
java -jar ~/dspot/dspot/target/dspot-1.1.1-SNAPSHOT-jar-with-dependencies.jar --test com.squareup.javapoet.AbstractTypesTest --cases getBasicTypeMirror --path-to-properties dspot.properties --amplifiers StatementAdd --clean --verbose
#+END_SRC
  + didn't yield anything, but didn't crash


*** Blocking Points
- [ ] Need to identify the main object that is interacted with
- [ ] null ~parentName~

*** Planned Work [8/15]
- [X] IELTS Speaking <2018-05-18 Fri 10:00>
- [X] IELTS <2018-05-19 Sat 08:30>
- [ ] Integrate WALA to compute a slice per new mutant
  + [ ] Adding as dependency
  + [ ] Use it
- [ ] Retrieve amplifications [0/1]
  + [ ] add categories of modifying amplifiers
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
- [X] +Retrieve mutants+
  + The [[https://github.com/STAMP-project/dspot/issues/122][issue]] on runtime info from PIT
  + [X] +Modify PIT to print the changed line+
  + [[https://groups.google.com/forum/#!searchin/pitusers/mutated$20line%7Csort:date/pitusers/7AMYsBAixPs/o_XGTqkpBgAJ]]
    - "Pitest mutates bytecode. Unfortunately there is no easy way to convert
      from bytecode to java source."
- [ ] Work on report [0/4]
  + [ ] add Java examples
  + [ ] describe thoroughly the oracle problem
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [ ] make a table for related works?
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
- [X] unspecify tests [5/5]
  + use a Set to avoid failing tests due to the ordering of assertions
  + [X] ~testBuildNewAssertWithComment(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testBuildNewAssert(fr.inria.diversify.dspot.assertGenerator.MethodsAssertGeneratorTest): expected:<....junit.Assert.assert[True(((fr.inria.sample.ClassWithBoolean)cl).getTrue());(..)~
  + [X] ~testNoInstrumentationOnGeneratedObject(fr.inria.diversify.dspot.assertGenerator.AssertGeneratorHelperTest): expected:<..."test_sd6__1");(..)~
  + [X] ~testWithLoop(fr.inria.diversify.dspot.amplifier.StatementAddTest): expected:<...compute(0);(..)~
  + [X] +find others that could fail+
  + problem of readability when it fails
  + [[https://github.com/STAMP-project/dspot/pull/413]]
- [-] improvements [3/6]
  + [ ] "The new test can detect if toBuilder returns XXX instead of the regular
    value. The original test 'toto' could not detect this fault"
    where XXX is the value injected by the mutation
    - isn't really suitable for other kinds of mutants, and even for
      return-related mutants as what might be interesting is that they change
      the state of the SUT, but don't have a direct relation with the test case
  + [ ] also consider when mutation modifies the state and is detected later
  + [ ] Don't name mutators, only explain the transformation instance
    (i.e. mutator category?)
  + [X] don't talk about 'new detectable bugs' but about 'assess more behavior
    than original' and/or 'reach more paths than original'
  + [X] For example, "this new test assesses more behavior as the original: it
    can detect 5 changes in the source code that the original test could not
    detect:" and then show the changes (diffs)
  + [X] handle ~try/catch~
- [ ] Make better names for generated tests
- [X] Don't add ~try/catch~ when src_java[:exports code]{@Test(expected = IllegalArgumentException.class)}
  + [[https://github.com/STAMP-project/dspot/pull/433]]
  + the problem was that the annotation was overridden to add the timeout
- [X] send email for the job at fb
- [ ] differentiate assert, trycatch, +and test annotations+
- [X] add DSpot and +PIT+ forks as submodules
- [X] reproduce DSpot on xwiki-commons
  + asked for the xwiki-commons version


** DONE Week 16 <2018-05-21 Mon>--<2018-05-27 Sun>
*** Things Done
- Observations from [[https://github.com/sbihel/xwiki-commons/issues/1]]
  + *very* long names for new variables
- looked at =mutations.csv= (had to comment out the deletion of the pit results)
  + collected description from the xml report
- run on javapoet
#+BEGIN_SRC sh
java -jar ~/internship_amplification/dspot/dspot/target/dspot-1.1.1-SNAPSHOT-jar-with-dependencies.jar --path-to-properties dspot.properties --amplifiers MethodAdd:StatementAdd:NumberLiteralAmplifier:BooleanLiteralAmplifier --test-criterion PitMutantScoreSelector --test com.squareup.javapoet.TypeSpecTest --clean --verbose
#+END_SRC
  + didn't finish, got stuck
  + rerun with only =MethodAdd= -> GC overhead
  + rerun with only =MethodAdd= on UtilTest, no new tests
#+BEGIN_SRC sh
java -jar ~/internship_amplification/dspot/dspot/target/dspot-1.1.1-SNAPSHOT-jar-with-dependencies.jar --path-to-properties dspot.properties --amplifiers MethodAdd:StatementAdd:NumberLiteralAmplifier:BooleanLiteralAmplifier --test-criterion PitMutantScoreSelector --test com.squareup.javapoet.NameAllocatorTest --clean --verbose
#+END_SRC
  + worked, in 33min

*** Blocking Points
- [X] +Need to identify the main object that is interacted with+
- [X] null ~parentName~
  + not with =IdentityHashMap=
  + we'll keep that as we have no time to care about performances
- [ ] total number of assertions not matching
  + sometimes total is higher (minimisation not caught?)
  + sometimes total is lower
- [ ] no direct parent, for the ordering, so it is useless

*** Planned Work [12/23]
- [X] +Integrate WALA to compute a slice per new mutant+
  + [X] +Adding as dependency+
  + [X] +Use it+
- [X] Retrieve amplifications [1/1]
  + [X] +add categories of modifying amplifiers+
    - MODIFY LITERALS
    - MODIFY INTERACTIONS
    - yeah, no, maybe in the future
- [ ] Work on report [0/10]
  + [ ] add Java examples
  + [ ] describe thoroughly the oracle problem
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [ ] make a table for related works?
  + [ ] DSpot
    - [ ] the process of removing all assertions, then adding some, then
      input-amplification, then assertions
  + [ ] What have I done [0/3]
    - [ ] Conceptual contribution
      + variables created through a-amplification, that aren't useful for an
      assertion but that raise exceptions
      + long names, so NL simple syntheses are nice
    - [ ] Implementation
      + lines
    - [ ] Aside stuff
      + [ ] bugs encountered and fixed
  + [ ] Problems encountered
    - ideally we would like to have infos about whether a mutant modifies the
      state of the CUT or if it's something else
  + [ ] Evaluation
    + low overhead
    + it works (e.g. right total number of asserts)
  + [ ] Go through ... to remember what I did [0/4]
    - [ ] previous journal entries
    - [ ] emails
    - [ ] commits
    - [ ] functions of =pr_message_gen=
  + [ ] Reduce the number of references
- [-] ask for opinions (e.g. Simon U, XWiki, SAT4J)
  + [-] XWiki
    - [X] get all infos necessary
    - [X] reproduce experiment
    - [ ] ask for feedback
  + [ ] others
- [X] improvements [7/7]
  + [X] "The new test can detect if toBuilder returns XXX instead of the regular
    value. The original test 'toto' could not detect this fault"
    where XXX is the value injected by the mutation
    - isn't really suitable for other kinds of mutants, and even for
      return-related mutants as what might be interesting is that they change
      the state of the SUT, but don't have a direct relation with the test case
    - yeah, no.
  + [X] +also consider when mutation modifies the state and is detected later+
  + [X] Don't name mutators, only explain the transformation instance
    (i.e. mutator category?)
    - PIT description
  + [X] fix assignments with multiple lines
  + [X] useless parent of i-amp
  + [X] useless try/catch diff
  + [X] use 'an' for 1
- [ ] Fix names of generated tests and originals'
- [X] +differentiate assert, trycatch?+
  + do it in the message generator
- [X] Meeting Benoit <2018-05-21 Mon 16:00>
  + state of the tool
  + state of the report
  + what's planned for the coming month
  +
  + getters, is, toString only
  + remove mutants killed by the original
  + distinguish improving coverage or new capacity
- [X] Group assertions for the variable they're testing
- [ ] realise that only getters, is, and toString are used
- [ ] realise that asserts always go through ~getFactory~
- [ ] remove redundant mutants
- [ ] differentiate additional coverage to new behaviours
- [X] natural language description of assertions that target a variable
- [X] add folds for overwhelming details
- [X] Order tests, starting with the older one first
- [ ] Sometimes an assertion for a variable directly test the variable (it can
  be simply a string), so no need to find a method call
- [ ] Reformat assertions for a variable when there is only one assert for this
  variable
- [X] Reformat multiple identical assertions
- [X] New useless assert variable should be reported differently as inputs
- [ ] Target grouping should be based on variable (not variable#method)
- [X] Add whole test snippet for development
- [ ] Maybe remaining assertions should be treated like assertions that check a
  variable that was generated at the same time


** DONE Week 17 <2018-05-28 Mon>--<2018-06-03 Sun>
*** Things Done
- =AmplificationLog= with strings as key to avoid losing parents
  + no wait, shouldn't change anything, because no weak keys

*** Blocking Points
- [ ] total number of assertions not matching
  + sometimes total is higher (minimisation not caught?)
  + sometimes total is lower
  + not using DSpot's report number for now
- [X] +no direct parent, for the ordering, so it is useless+ +wrong parenting+
  no direct parent
  + using all =String=s for the parenting map doesn't work either
  + =ampTestToParentName= was cleared before report
    - nope
  + the problem was in pr_gen?? I'm the worst
  + still no direct parent
  + no direct parent because only a-amp(?)
    - yes, so two cases
      1. only better oracle
      2. also better inputs
- [ ] Why, sometime, are there amplified tests with 3 iterations of amplifications
  with no logged parent, and sometimes there is 1 parent?
- [ ] Speaking about different behaviour and different oracle defeats the goal
  of keeping things very simple
- [ ] Amplifications log not kept for every test?

*** Planned Work [1/12]
- [ ] Work on report [0/10]
  + [ ] add Java examples
  + [ ] describe thoroughly the oracle problem
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [ ] make a table for related works?
  + [ ] DSpot
    - [ ] the process of removing all assertions, then adding some, then
      input-amplification, then assertions
    - [ ] iterations
  + [ ] What have I done [0/3]
    - [ ] Conceptual contribution
      + variables created through a-amplification, that aren't useful for an
      assertion but that raise exceptions
      + long names, so NL simple syntheses are nice
      + identifying killing assertions isn't so great because it's not
        necessarily direct
    - [ ] Implementation
      + lines
    - [ ] Aside stuff
      + [ ] bugs encountered and fixed
  + [ ] Problems encountered
    - ideally we would like to have infos about whether a mutant modifies the
      state of the CUT or if it's something else
  + [ ] Evaluation
    + low overhead
    + it works (e.g. right total number of asserts)
  + [ ] Go through ... to remember what I did [0/4]
    - [ ] previous journal entries
    - [ ] emails
    - [ ] commits
    - [ ] functions of =pr_message_gen=
  + [ ] Reduce the number of references
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
  + [ ] XWiki
    - [ ] ask for feedback
  + [ ] others
- [X] +Fix names of generated tests and originals'+
- [ ] realise that only getters, is, and toString are used
- [ ] realise that asserts always go through ~getFactory~
- [ ] remove redundant mutants
- [ ] differentiate additional coverage to new behaviours
- [ ] Sometimes an assertion for a variable directly test the variable (it can
  be simply a string), so no need to find a method call
- [ ] Reformat assertions for a variable when there is only one assert for this
  variable
- [ ] Target grouping should be based on variable (not variable#method)
- [ ] Maybe remaining assertions should be treated like assertions that check a
  variable that was generated at the same time
- [ ] In the case of a useless (alone) i-amp, still need to recover these to
  explain the a-amp that follow


** DONE Week 18 <2018-06-04 Mon>--<2018-06-10 Sun>
*** Things Done
- run on xwiki cipher [xwiki-commons-10.0] [VisualVM 1.4.1]
  + =--amplifiers MethodAdd --test-criterion PitMutantScoreSelector --clean --verbose=
  + fork: 20m27s, 21m40s 884MB max used heap
  + main master: 19m52s, 21m56s 834MB max used heap
- run on javapoet
  + =--amplifiers MethodAdd:StatementAdd:NumberLiteralAmplifier:BooleanLiteralAmplifier --test-criterion PitMutantScoreSelector --test com.squareup.javapoet.NameAllocatorTest --clean --verbose=
  + fork:
  + main master:
  + =Failed to execute goal org.pitest:pitest-maven:1.3.0:mutationCoverage (default-cli) on project javapoet: Execution default-cli of goal org.pitest:pitest-maven:1.3.0:mutationCoverage failed: All tests did not pass without mutation when calculating line coverage. Mutation testing requires a green suite.=

*** Blocking Points
- [ ] total number of assertions not matching
  + sometimes total is higher (minimisation not caught?)
  + sometimes total is lower
  + not using DSpot's report number for now
- [X] Why, sometime, are there amplified tests with 3 iterations of amplifications
  with no logged parent, and sometimes there is 1 parent?
  + not true
- [ ] Speaking about different behaviour and different oracle defeats the goal
  of keeping things very simple
- [X] Amplifications log not kept for every test?
  + it was a problem with using the name of the class when ~ampliedTest == null~

*** Planned Work [4/14]
- [X] Report <2018-06-08 Fri 12:00>
- [X] Renew SL access card <2018-06-07 Thu>
- [-] Work on report [8/11]
  + [X] add Java examples
  + [X] +describe thoroughly the oracle problem+
  + [ ] overspecification
    - [[http://jasonrudolph.com/blog/2008/07/01/testing-anti-patterns-overspecification/]]
  + [X] +make a table for related works?+
  + [X] DSpot
    - [X] the process of removing all assertions, then adding some, then
      input-amplification, then assertions
    - [X] iterations
    - [X] minimisation
    - [X] heavy memory usage
  + [-] What have I done [2/3]
    - [-] Conceptual contribution
      + [X] variables created through a-amplification, that aren't useful for an
        assertion but that raise exceptions
      + [ ] long names, so NL simple syntheses are nice
      + [X] identifying killing assertions isn't so great because it's not
        necessarily direct
      + [X] assume that the original is known and understood by the developer?
      + [X] better oracle and/or new behaviour
    - [X] Implementation
      + lines
      + aimed at GitHub, but could be used for GitLab, but not for BitBucket
        (because of the html of the fold blocks)
    - [X] Aside stuff
      + [X] bugs encountered and fixed
  + [X] Problems encountered
    - [X] ideally we would like to have infos about whether a mutant modifies
      the state of the CUT or if it's something else
    - [X] balance between complex information and information simplified to a
      useless point
    - [X] speaking of mutants by talking about changes
    - [X] very hard to log everything that happens, especially after with
      minisation where the border between original and amplification thins down
  + [X] Evaluation
    + low overhead
    + it works (e.g. right total number of asserts)
  + [ ] Go through ... to remember what I did [0/4]
    - [ ] previous journal entries
    - [ ] emails
    - [ ] commits
    - [ ] functions of =pr_message_gen=
  + [X] +Reduce the number of references+
  + [X] Screenshots
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
  + [ ] XWiki
    - [ ] ask for feedback
  + [ ] others
- [ ] realise that only getters, is, and toString are used
- [ ] realise that asserts always go through ~getFactory~
- [ ] remove redundant mutants
- [ ] differentiate additional coverage to new behaviours
- [ ] Sometimes an assertion for a variable directly test the variable (it can
  be simply a string), so no need to find a method call
- [ ] Reformat assertions for a variable when there is only one assert for this
  variable
- [ ] Target grouping should be based on variable (not variable#method)
- [ ] Maybe remaining assertions should be treated like assertions that check a
  variable that was generated at the same time
- [X] In the case of a useless (alone) i-amp, still need to recover these to
  explain the a-amp that follow
- [X] Update =AmplificationLog= after minimization


** DONE Week 19 <2018-06-11 Mon>--<2018-06-17 Sun>
*** Things Done

*** Blocking Points
- [ ] total number of assertions not matching
  + sometimes total is higher (minimisation not caught?)
  + sometimes total is lower
  + not using DSpot's report number for now
- [ ] Speaking about different behaviour and different oracle defeats the goal
  of keeping things very simple

*** Planned Work [2/12]
- [ ] ask for opinions (e.g. Simon U, XWiki, SAT4J)
  + [ ] XWiki
    - [ ] ask for feedback
  + [ ] others
- [ ] realise that only getters, is, and toString are used
- [ ] realise that asserts always go through ~getFactory~
- [ ] remove redundant mutants
- [ ] differentiate additional coverage to new behaviours
- [ ] Sometimes an assertion for a variable directly test the variable (it can
  be simply a string), so no need to find a method call
- [ ] Reformat assertions for a variable when there is only one assert for this
  variable
- [ ] Target grouping should be based on variable (not variable#method)
- [ ] Maybe remaining assertions should be treated like assertions that check a
  variable that was generated at the same time
- [ ] Work on slides [0/1]
  + [ ] add references
- [X] Return keys to Kista
- [X] Meeting Benoit <2018-06-13 Wed 14:00>
  + +Kista keys?+


** DONE Week 20 <2018-06-18 Mon>--<2018-06-24 Sun>
*** Things Done
- Threw printed papers. Hard to see all these promising annotations go.

*** Blocking Points

*** Planned Work [3/4]
- [X] +Defense Rehearsal @ ENS+ <2018-06-22 Fri>
- [X] Hand in grant form <2018-06-21 Thu>
- [X] Return keys
  + [X] office
  + [X] apartment
- [ ] Work on slides [0/2]
  + [ ] add references
  + [ ] a bit of state of the art


** DONE Week 21 <2018-06-25 Mon>--<2018-06-27 Wed>
*** Things Done

*** Blocking Points

*** Planned Work [2/2]
- [X] Defense <2018-06-27 Wed 14:00> @ Jersey Room
- [X] Work on slides [2/2]
  + [X] add references
  + [X] a bit of state of the art


* Conclusion
